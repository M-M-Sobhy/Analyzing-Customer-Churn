{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwyxAdQrJg810fm7I4/tOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84d40a658fad4850b22fe17ed960983b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59206ec049ba430aadbd05bd6a8d215b",
              "IPY_MODEL_70733829a689435594831fcc8fbc2bd1",
              "IPY_MODEL_3aadc5625a10471b93d99bb3737122c9"
            ],
            "layout": "IPY_MODEL_06c35119875d40d8a3228eef19a2ca43"
          }
        },
        "59206ec049ba430aadbd05bd6a8d215b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ab997abc194834b0939e5ac3558fdb",
            "placeholder": "​",
            "style": "IPY_MODEL_3deb1a8408954e9a97581ea56684d3d2",
            "value": "  0%"
          }
        },
        "70733829a689435594831fcc8fbc2bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec4f33ae62cd4a60aa513f8c57d1155c",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e22f700e410461d9e4bc17acd9c6518",
            "value": 6
          }
        },
        "3aadc5625a10471b93d99bb3737122c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb69e75960547e0a659d1e7f8c7e253",
            "placeholder": "​",
            "style": "IPY_MODEL_6706a402d0744aa1b085419323b2d77b",
            "value": " 6/2000 [01:24&lt;6:33:10, 11.83s/it]"
          }
        },
        "06c35119875d40d8a3228eef19a2ca43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ab997abc194834b0939e5ac3558fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3deb1a8408954e9a97581ea56684d3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec4f33ae62cd4a60aa513f8c57d1155c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e22f700e410461d9e4bc17acd9c6518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cb69e75960547e0a659d1e7f8c7e253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6706a402d0744aa1b085419323b2d77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1043774cd3ca4a09919eb1b15cbb80ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c3164343fe24bf6a41e15fd3a34a616",
              "IPY_MODEL_78ed8268690a4b0d802fdb12ae7f7b23",
              "IPY_MODEL_4b2e1f3a840c440f956ad54eb3a509d1"
            ],
            "layout": "IPY_MODEL_60ce60ba22be4b559fb548418b42b0f5"
          }
        },
        "8c3164343fe24bf6a41e15fd3a34a616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562972d0449d44cfa88e6bdc7c87b595",
            "placeholder": "​",
            "style": "IPY_MODEL_7f2220a53d9342d1b6519a33c467f3d3",
            "value": "100%"
          }
        },
        "78ed8268690a4b0d802fdb12ae7f7b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8bf507e5d344ab6a9639dbefb3e3337",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0c400a3051a4937b1e040507c0bd42e",
            "value": 300
          }
        },
        "4b2e1f3a840c440f956ad54eb3a509d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd3e963388a34c8f9472bbcc68137368",
            "placeholder": "​",
            "style": "IPY_MODEL_b87e1143f6bf43518755b9d58eb91281",
            "value": " 300/300 [04:32&lt;00:00,  1.16it/s]"
          }
        },
        "60ce60ba22be4b559fb548418b42b0f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562972d0449d44cfa88e6bdc7c87b595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2220a53d9342d1b6519a33c467f3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8bf507e5d344ab6a9639dbefb3e3337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c400a3051a4937b1e040507c0bd42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd3e963388a34c8f9472bbcc68137368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87e1143f6bf43518755b9d58eb91281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ea9eb59cf404ac198e658e9532ba3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11b586670f5746cba724a1cb5f825920",
              "IPY_MODEL_0e2b49c4be12413f8e82e94faa551a8f",
              "IPY_MODEL_ef9a16b8330642079dc5b085a8fdd7ef"
            ],
            "layout": "IPY_MODEL_7a56dc752da34de6a9535b6a3c3e1d9c"
          }
        },
        "11b586670f5746cba724a1cb5f825920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89824b6eb44a408ab93424a17ac5a63f",
            "placeholder": "​",
            "style": "IPY_MODEL_5e0da990cde74269ad1fd9e304802522",
            "value": "100%"
          }
        },
        "0e2b49c4be12413f8e82e94faa551a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7613a482425424b98270825d78fa26f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0528d5b5d604b7e9b68424802bf18e9",
            "value": 1
          }
        },
        "ef9a16b8330642079dc5b085a8fdd7ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be0e0e9b51ae4ed0b6f69283b2dd35fb",
            "placeholder": "​",
            "style": "IPY_MODEL_8d9b6f3b36e440e8a9a18b0a9e6cec20",
            "value": " 1/1 [00:00&lt;00:00,  1.49it/s]"
          }
        },
        "7a56dc752da34de6a9535b6a3c3e1d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89824b6eb44a408ab93424a17ac5a63f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0da990cde74269ad1fd9e304802522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7613a482425424b98270825d78fa26f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0528d5b5d604b7e9b68424802bf18e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be0e0e9b51ae4ed0b6f69283b2dd35fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9b6f3b36e440e8a9a18b0a9e6cec20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06112468c06340fca78119e62cda0b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4623c1e33ef402997e8c0cefa774403",
              "IPY_MODEL_12b95b8c3f2d4af88782442b468ad807",
              "IPY_MODEL_33da78ce82914397a04db1dcd72652bf"
            ],
            "layout": "IPY_MODEL_50725bf232a14380b117ba18cfc488f6"
          }
        },
        "e4623c1e33ef402997e8c0cefa774403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fbe3d5d18d7436983e3df24aad5f383",
            "placeholder": "​",
            "style": "IPY_MODEL_927a70937d464868adf3af4d8818677c",
            "value": "100%"
          }
        },
        "12b95b8c3f2d4af88782442b468ad807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff41f0357ce45618e9eb7eeaf99a85f",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7c397013f09495baefee1741add0405",
            "value": 300
          }
        },
        "33da78ce82914397a04db1dcd72652bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b05227b2a441d582a01ddb86dc90ca",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f0758221344585aa2b5ffe6b1ca87f",
            "value": " 300/300 [03:39&lt;00:00,  1.14it/s]"
          }
        },
        "50725bf232a14380b117ba18cfc488f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fbe3d5d18d7436983e3df24aad5f383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927a70937d464868adf3af4d8818677c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ff41f0357ce45618e9eb7eeaf99a85f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c397013f09495baefee1741add0405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10b05227b2a441d582a01ddb86dc90ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f0758221344585aa2b5ffe6b1ca87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "147a4922a0914933a38396f809a3de46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04754e03613f4392945ef63acd11d1fd",
              "IPY_MODEL_0efe3cbe56244280a162414a4fd4dbf0",
              "IPY_MODEL_ec3891872b2b45afa518c22c8488f321"
            ],
            "layout": "IPY_MODEL_b9f96078a7bf48b8b1e49546d254f222"
          }
        },
        "04754e03613f4392945ef63acd11d1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941aa790aff04172a72bb4184433553a",
            "placeholder": "​",
            "style": "IPY_MODEL_734de97d46cf46828febccd640662d51",
            "value": "100%"
          }
        },
        "0efe3cbe56244280a162414a4fd4dbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86340a6dbc849afa2bc8369bddcde5c",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f0a79cd3f904048b760936ea646c601",
            "value": 300
          }
        },
        "ec3891872b2b45afa518c22c8488f321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd72847af3ee450691d0a14b856725ca",
            "placeholder": "​",
            "style": "IPY_MODEL_9bf3e14bd0354287ba28bb785d756673",
            "value": " 300/300 [05:14&lt;00:00,  1.12it/s]"
          }
        },
        "b9f96078a7bf48b8b1e49546d254f222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941aa790aff04172a72bb4184433553a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734de97d46cf46828febccd640662d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86340a6dbc849afa2bc8369bddcde5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0a79cd3f904048b760936ea646c601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd72847af3ee450691d0a14b856725ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf3e14bd0354287ba28bb785d756673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-M-Sobhy/Analyzing-Customer-Churn/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEiWoSBg-VL3",
        "outputId": "34ae2de8-4c57-464d-ac71-3d7cfd1b1014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb', 'mlp']\n",
            "→ Building OOF & test meta matrices...\n",
            "[xgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6321\n",
            "[LightGBM] [Info] Number of positive: 1121, number of negative: 3104\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 4225, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265325 -> initscore=-1.018470\n",
            "[LightGBM] [Info] Start training from score -1.018470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1121, number of negative: 3104\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 4225, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265325 -> initscore=-1.018470\n",
            "[LightGBM] [Info] Start training from score -1.018470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1122, number of negative: 3104\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 4226, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265499 -> initscore=-1.017579\n",
            "[LightGBM] [Info] Start training from score -1.017579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1122, number of negative: 3104\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 4226, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265499 -> initscore=-1.017579\n",
            "[LightGBM] [Info] Start training from score -1.017579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1122, number of negative: 3104\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 4226, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265499 -> initscore=-1.017579\n",
            "[LightGBM] [Info] Start training from score -1.017579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1402, number of negative: 3880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 667\n",
            "[LightGBM] [Info] Number of data points in the train set: 5282, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265430 -> initscore=-1.017935\n",
            "[LightGBM] [Info] Start training from score -1.017935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8204 | PR-AUC=0.6139\n",
            "[cat] OOF ROC-AUC=0.8341 | PR-AUC=0.6457\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6159\n",
            "[hgb] OOF ROC-AUC=0.8238 | PR-AUC=0.6229\n",
            "[mlp] OOF ROC-AUC=0.7603 | PR-AUC=0.5848\n",
            "{\n",
            "  \"oof_base_scores\": {\n",
            "    \"xgb\": {\n",
            "      \"oof_roc_auc_mean\": 0.8254507754233513,\n",
            "      \"oof_pr_auc_mean\": 0.6321342362150559\n",
            "    },\n",
            "    \"lgb\": {\n",
            "      \"oof_roc_auc_mean\": 0.8204364442423702,\n",
            "      \"oof_pr_auc_mean\": 0.6138892325593518\n",
            "    },\n",
            "    \"cat\": {\n",
            "      \"oof_roc_auc_mean\": 0.8341403633535815,\n",
            "      \"oof_pr_auc_mean\": 0.6456745455349153\n",
            "    },\n",
            "    \"rf\": {\n",
            "      \"oof_roc_auc_mean\": 0.8214415530217665,\n",
            "      \"oof_pr_auc_mean\": 0.6158901692851213\n",
            "    },\n",
            "    \"hgb\": {\n",
            "      \"oof_roc_auc_mean\": 0.8238124462785444,\n",
            "      \"oof_pr_auc_mean\": 0.6228601950922066\n",
            "    },\n",
            "    \"mlp\": {\n",
            "      \"oof_roc_auc_mean\": 0.7602676802551376,\n",
            "      \"oof_pr_auc_mean\": 0.5848197350110097\n",
            "    }\n",
            "  },\n",
            "  \"meta_scores\": {\n",
            "    \"roc_auc\": 0.8396834012358143,\n",
            "    \"pr_auc\": 0.6437354414400763,\n",
            "    \"f1\": 0.6291079812206573\n",
            "  },\n",
            "  \"best_threshold\": 0.22499999999999998,\n",
            "  \"outputs\": {\n",
            "    \"metrics_json\": \"outputs/metrics.json\",\n",
            "    \"roc_curve\": \"outputs/roc_curve_meta.png\",\n",
            "    \"pr_curve\": \"outputs/pr_curve_meta.png\",\n",
            "    \"model_bundle\": \"outputs/stacking_bundle.pkl\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# %% ONE-CELL: Telco Churn Stacking (XGB + LGBM + CatBoost + RF + HGB + MLP) -> Meta LogisticRegression\n",
        "# Works out-of-the-box on Google Colab with a single cell.\n",
        "\n",
        "# 0) Install deps (quiet)\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost matplotlib joblib\n",
        "\n",
        "# 1) Imports\n",
        "import os, json, sys, math, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, roc_curve, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 2) Config\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"   # ← لو اسم الملف مختلف عدّله هنا\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3) Locate dataset anywhere in the left pane (recursively)\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Make sure it is visible on the left panel.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# 4) Load & basic clean\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found in the CSV.\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# 5) Build a preprocessor (OHE for all non-numerics)\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 6) Split train/test once (temporal info not present in Telco)\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# 7) Define base models (diverse)\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    return {\n",
        "        \"xgb\": XGBClassifier(\n",
        "            n_estimators=400, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\"\n",
        "        ),\n",
        "        \"lgb\": LGBMClassifier(\n",
        "            n_estimators=500, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"cat\": CatBoostClassifier(\n",
        "            iterations=500, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"rf\": RandomForestClassifier(n_estimators=350, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "        \"hgb\": HistGradientBoostingClassifier(max_iter=350, learning_rate=0.08, random_state=RANDOM_STATE),\n",
        "        \"mlp\": MLPClassifier(hidden_layer_sizes=(128,64), activation=\"relu\",\n",
        "                             alpha=1e-3, max_iter=250, random_state=RANDOM_STATE),\n",
        "    }\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "M = len(base_names)\n",
        "print(f\"Base learners: {base_names}\")\n",
        "\n",
        "# 8) OOF stacking function: builds level-1 OOF matrix for meta-learner\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict[str, Dict[str, float]], List[Pipeline]]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores: Dict[str, Dict[str, float]] = {}\n",
        "    fitted_full_pipes: List[Pipeline] = []\n",
        "\n",
        "    # For consistent indexing\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline(steps=[\n",
        "                (\"pre\", preprocessor),\n",
        "                (\"clf\", clf)\n",
        "            ])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx] = p_va\n",
        "            # fold metrics (on OOF)\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        # OOF scores aggregated\n",
        "        aucs = [a for a, p in fold_scores]\n",
        "        prs  = [p for a, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        # Fit FULL train for test predictions\n",
        "        full_pipe = Pipeline(steps=[\n",
        "            (\"pre\", preprocessor),\n",
        "            (\"clf\", clf.__class__(**clf.get_params()))  # fresh clone with same params\n",
        "        ])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        test_meta[:, j] = full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | \"\n",
        "              f\"PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Building OOF & test meta matrices...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# 9) Meta-learner (Logistic Regression) trained on OOF\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# 10) Evaluate on test (using level-1 test_matrix)\n",
        "y_prob = meta.predict_proba(test_matrix)[:,1]\n",
        "y_hat  = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "meta_scores = {\n",
        "    \"roc_auc\": float(roc_auc_score(y_test, y_prob)),\n",
        "    \"pr_auc\":  float(average_precision_score(y_test, y_prob)),\n",
        "    \"f1\":      float(f1_score(y_test, y_hat))\n",
        "}\n",
        "\n",
        "# 11) Plots\n",
        "def plot_curves(y_true, y_score, out_dir: Path):\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=\"Meta ROC\")\n",
        "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve (Meta-Learner)\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir/\"roc_curve_meta.png\", dpi=160); plt.close()\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, label=\"Meta PR\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve (Meta-Learner)\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir/\"pr_curve_meta.png\", dpi=160); plt.close()\n",
        "\n",
        "plot_curves(y_test, y_prob, OUT_DIR)\n",
        "\n",
        "# 12) Business-aware threshold (optional, quick)\n",
        "def optimal_profit_threshold(y_true, y_score, reward_tp=50.0, cost_fp=10.0, cost_fn=20.0):\n",
        "    best_t, best_p = 0.5, -1e18\n",
        "    for t in np.linspace(0.05, 0.95, 181):\n",
        "        y_pred = (y_score >= t).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        profit = tp*reward_tp - fp*cost_fp - fn*cost_fn\n",
        "        if profit > best_p:\n",
        "            best_p, best_t = profit, t\n",
        "    return float(best_t), float(best_p)\n",
        "\n",
        "best_t, best_profit = optimal_profit_threshold(y_test, y_prob)\n",
        "\n",
        "# 13) Save artifacts (model = meta + fitted base pipes)\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_scores\": meta_scores,\n",
        "    \"best_threshold\": best_t,\n",
        "    \"best_threshold_profit\": best_profit,\n",
        "    \"base_models_order\": base_names\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "\n",
        "# Save a single pickle with everything needed for later inference\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,   # list of (preprocessor+model) pipelines fitted on full train\n",
        "    \"meta\": meta,                      # fitted meta-learner\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"oof_base_scores\": base_oof_scores,\n",
        "    \"meta_scores\": meta_scores,\n",
        "    \"best_threshold\": best_t,\n",
        "    \"outputs\": {\n",
        "        \"metrics_json\": str(OUT_DIR/\"metrics.json\"),\n",
        "        \"roc_curve\": str(OUT_DIR/\"roc_curve_meta.png\"),\n",
        "        \"pr_curve\": str(OUT_DIR/\"pr_curve_meta.png\"),\n",
        "        \"model_bundle\": str(OUT_DIR/\"stacking_bundle.pkl\")\n",
        "    }\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Compare CatBoost (single model) vs Stacking on the SAME test split\n",
        "# This cell now includes the necessary steps to generate the 'outputs/stacking_bundle.pkl' file\n",
        "# by incorporating relevant code from the previous cell.\n",
        "\n",
        "# 0) Install deps (quiet) - Included from previous cell\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost matplotlib joblib\n",
        "\n",
        "# 1) Imports - Included from previous cell\n",
        "import os, json, sys, math, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, roc_curve, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from tqdm.auto import trange\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 2) Config - Included from previous cell\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3) Locate dataset anywhere in the left pane (recursively) - Included from previous cell\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Make sure it is visible on the left panel.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# 4) Load & basic clean - Included from previous cell\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found in the CSV.\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# 5) Build a preprocessor (OHE for all non-numerics) - Included from previous cell\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 6) Split train/test once (temporal info not present in Telco) - Included from previous cell\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# 7) Define base models (diverse) - Included from previous cell\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    return {\n",
        "        \"xgb\": XGBClassifier(\n",
        "            n_estimators=400, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\"\n",
        "        ),\n",
        "        \"lgb\": LGBMClassifier(\n",
        "            n_estimators=500, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"cat\": CatBoostClassifier(\n",
        "            iterations=500, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"rf\": RandomForestClassifier(n_estimators=350, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "        \"hgb\": HistGradientBoostingClassifier(max_iter=350, learning_rate=0.08, random_state=RANDOM_STATE),\n",
        "        \"mlp\": MLPClassifier(hidden_layer_sizes=(128,64), activation=\"relu\",\n",
        "                             alpha=1e-3, max_iter=250, random_state=RANDOM_STATE),\n",
        "    }\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "M = len(base_names)\n",
        "print(f\"Base learners: {base_names}\")\n",
        "\n",
        "# 8) OOF stacking function: builds level-1 OOF matrix for meta-learner - Included from previous cell\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict[str, Dict[str, float]], List[Pipeline]]:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores: Dict[str, Dict[str, float]] = {}\n",
        "    fitted_full_pipes: List[Pipeline] = []\n",
        "\n",
        "    # For consistent indexing\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline(steps=[\n",
        "                (\"pre\", preprocessor),\n",
        "                (\"clf\", clf)\n",
        "            ])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx] = p_va\n",
        "            # fold metrics (on OOF)\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        # OOF scores aggregated\n",
        "        aucs = [a for a, p in fold_scores]\n",
        "        prs  = [p for a, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        # Fit FULL train for test predictions\n",
        "        full_pipe = Pipeline(steps=[\n",
        "            (\"pre\", preprocessor),\n",
        "            (\"clf\", clf.__class__(**clf.get_params()))  # fresh clone with same params\n",
        "        ])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        test_meta[:, j] = full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | \"\n",
        "              f\"PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Building OOF & test meta matrices...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# 9) Meta-learner (Logistic Regression) trained on OOF - Included from previous cell\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# 10) Evaluate on test (using level-1 test_matrix) - Included from previous cell\n",
        "y_prob = meta.predict_proba(test_matrix)[:,1]\n",
        "y_hat  = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "meta_scores = {\n",
        "    \"roc_auc\": float(roc_auc_score(y_test, y_prob)),\n",
        "    \"pr_auc\":  float(average_precision_score(y_test, y_prob)),\n",
        "    \"f1\":      float(f1_score(y_test, y_hat))\n",
        "}\n",
        "\n",
        "# 11) Plots - Included from previous cell\n",
        "def plot_curves(y_true, y_score, out_dir: Path):\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=\"Meta ROC\")\n",
        "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve (Meta-Learner)\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir/\"roc_curve_meta.png\", dpi=160); plt.close()\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, label=\"Meta PR\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve (Meta-Learner)\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir/\"pr_curve_meta.png\", dpi=160); plt.close()\n",
        "\n",
        "plot_curves(y_test, y_prob, OUT_DIR)\n",
        "\n",
        "# 12) Business-aware threshold (optional, quick) - Included from previous cell\n",
        "def optimal_profit_threshold(y_true, y_score, reward_tp=50.0, cost_fp=10.0, cost_fn=20.0):\n",
        "    best_t, best_p = 0.5, -1e18\n",
        "    for t in np.linspace(0.05, 0.95, 181):\n",
        "        y_pred = (y_score >= t).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        profit = tp*reward_tp - fp*cost_fp - fn*cost_fn\n",
        "        if profit > best_p:\n",
        "            best_p, best_t = profit, t\n",
        "    return float(best_t), float(best_p)\n",
        "\n",
        "best_t, best_profit = optimal_profit_threshold(y_test, y_prob)\n",
        "\n",
        "# 13) Save artifacts (model = meta + fitted base pipes) - Included from previous cell\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_scores\": meta_scores,\n",
        "    \"best_threshold\": best_t,\n",
        "    \"best_threshold_profit\": best_profit,\n",
        "    \"base_models_order\": base_names\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "\n",
        "# Save a single pickle with everything needed for later inference - Included from previous cell\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,   # list of (preprocessor+model) pipelines fitted on full train\n",
        "    \"meta\": meta,                      # fitted meta-learner\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"oof_base_scores\": base_oof_scores,\n",
        "    \"meta_scores\": meta_scores,\n",
        "    \"best_threshold\": best_t,\n",
        "    \"outputs\": {\n",
        "        \"metrics_json\": str(OUT_DIR/\"metrics.json\"),\n",
        "        \"roc_curve\": str(OUT_DIR/\"roc_curve_meta.png\"),\n",
        "        \"pr_curve\": str(OUT_DIR/\"pr_curve_meta.png\"),\n",
        "        \"model_bundle\": str(OUT_DIR/\"stacking_bundle.pkl\")\n",
        "    }\n",
        "}, indent=2))\n",
        "\n",
        "# -------------------\n",
        "# 14) Load your trained bundle (stacking) and extract CatBoost pipe\n",
        "# -------------------\n",
        "# Removed redundant data loading and splitting from original cell\n",
        "# bundle = joblib.load(\"outputs/stacking_bundle.pkl\") # Already loaded above\n",
        "# base_pipes = bundle[\"base_pipes\"]          # fitted on FULL train # Already loaded above\n",
        "# base_order = bundle[\"base_order\"]          # e.g., ['xgb','lgb','cat','rf','hgb','mlp'] # Already loaded above\n",
        "# meta = bundle[\"meta\"] # Already loaded above\n",
        "\n",
        "# Find CatBoost pipe by name\n",
        "try:\n",
        "    cat_idx = base_order.index(\"cat\")\n",
        "except ValueError:\n",
        "    raise RuntimeError(\"Couldn't find 'cat' in base_order; check your base model names.\")\n",
        "\n",
        "pipe_cat = base_pipes[cat_idx]\n",
        "\n",
        "# -------------------\n",
        "# 15) Predict probabilities\n",
        "# -------------------\n",
        "# CatBoost alone:\n",
        "p_cat = pipe_cat.predict_proba(X_test_raw)[:,1] # Using X_test_raw from the combined code\n",
        "\n",
        "# Stacking:\n",
        "test_meta = np.column_stack([p.predict_proba(X_test_raw)[:,1] for p in base_pipes]) # Using X_test_raw from the combined code\n",
        "p_stack = meta.predict_proba(test_meta)[:,1]\n",
        "\n",
        "# Load profit-aware threshold you computed سابقاً (لو موجود)\n",
        "t_star = None\n",
        "mjson = Path(\"outputs/metrics.json\")\n",
        "if mjson.exists():\n",
        "    cfg = json.loads(mjson.read_text())\n",
        "    t_star = float(cfg.get(\"best_threshold\", 0.5))\n",
        "else:\n",
        "    t_star = 0.5\n",
        "\n",
        "# -------------------\n",
        "# 16) Metrics @probabilities (AUCs) + threshold sweep\n",
        "# -------------------\n",
        "def summarize_probs(y_true, y_score, name, thr=0.5):\n",
        "    roc = roc_auc_score(y_true, y_score)\n",
        "    pr  = average_precision_score(y_true, y_score)\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return {\n",
        "        \"model\": name, \"ROC_AUC\": roc, \"PR_AUC\": pr, \"F1@thr\": f1,\n",
        "        \"Precision@thr\": prec, \"Recall@thr\": rec, \"thr\": thr,\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn\n",
        "    }\n",
        "\n",
        "# Baseline at business threshold\n",
        "sum_cat   = summarize_probs(y_test, p_cat,   \"CatBoost\",  t_star) # Using y_test from the combined code\n",
        "sum_stack = summarize_probs(y_test, p_stack, \"Stacking\",  t_star) # Using y_test from the combined code\n",
        "summary_df = pd.DataFrame([sum_cat, sum_stack])\n",
        "print(\"== Summary @ business threshold ==\")\n",
        "display(summary_df)\n",
        "\n",
        "# Threshold sweep table\n",
        "thr_grid = np.linspace(0.05, 0.95, 37)\n",
        "def sweep_table(y_true, y_score, name):\n",
        "    rows = []\n",
        "    for t in thr_grid:\n",
        "        rows.append(summarize_probs(y_true, y_score, name, t))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "sweep_cat   = sweep_table(y_test, p_cat,   \"CatBoost\") # Using y_test from the combined code\n",
        "sweep_stack = sweep_table(y_test, p_stack, \"Stacking\") # Using y_test from the combined code\n",
        "\n",
        "# -------------------\n",
        "# 17) Profit curve (you can tweak costs/rewards)\n",
        "# -------------------\n",
        "REWARD_TP = 50.0\n",
        "COST_FP   = 10.0\n",
        "COST_FN   = 20.0\n",
        "\n",
        "def profit_at(y_true, y_score, thr):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tp*REWARD_TP - fp*COST_FP - fn*COST_FN\n",
        "\n",
        "def profit_curve(y_true, y_score):\n",
        "    return pd.DataFrame({\n",
        "        \"threshold\": thr_grid,\n",
        "        \"profit\": [profit_at(y_true, y_score, t) for t in thr_grid]\n",
        "    })\n",
        "\n",
        "profit_cat   = profit_curve(y_test, p_cat).assign(model=\"CatBoost\") # Using y_test from the combined code\n",
        "profit_stack = profit_curve(y_test, p_stack).assign(model=\"Stacking\") # Using y_test from the combined code\n",
        "profit_all = pd.concat([profit_cat, profit_stack], ignore_index=True)\n",
        "\n",
        "# Best profit per model\n",
        "best_cat   = profit_cat.loc[profit_cat[\"profit\"].idxmax()]\n",
        "best_stack = profit_stack.loc[profit_stack[\"profit\"].idxmax()]\n",
        "print(f\"Best profit CatBoost: thr={best_cat.threshold:.3f}, profit={best_cat.profit:.1f}\")\n",
        "print(f\"Best profit Stacking: thr={best_stack.threshold:.3f}, profit={best_stack.profit:.1f}\")\n",
        "\n",
        "# -------------------\n",
        "# 18) Bootstrap CIs for ROC-AUC and PR-AUC\n",
        "# -------------------\n",
        "def bootstrap_ci_auc(y_true, y_score, n=1000, alpha=0.05, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n_ = len(y_true)\n",
        "    rocs, prs = [], []\n",
        "    for _ in trange(n, leave=False):\n",
        "        idx = rng.integers(0, n_, n_)\n",
        "        yt = np.asarray(y_true)[idx]; ps = np.asarray(y_score)[idx]\n",
        "        # Guard for degenerate samples with single class\n",
        "        if len(np.unique(yt)) < 2:\n",
        "            continue\n",
        "        rocs.append(roc_auc_score(yt, ps))\n",
        "        prs.append(average_precision_score(yt, ps))\n",
        "    rocs = np.array(rocs); prs = np.array(prs)\n",
        "    lo = int((alpha/2)*len(rocs)); hi = int((1-alpha/2)*len(rocs))\n",
        "    rocs_ci = np.sort(rocs)[[lo, hi-1]]\n",
        "    prs_ci  = np.sort(prs)[[lo, hi-1]]\n",
        "    return {\n",
        "        \"ROC_AUC\": (float(np.mean(rocs)), float(rocs_ci[0]), float(rocs_ci[1])),\n",
        "        \"PR_AUC\":  float(np.mean(prs)),  float(prs_ci[0]),  float(prs_ci[1])),\n",
        "        \"N\": int(len(rocs))\n",
        "    }\n",
        "\n",
        "ci_cat   = bootstrap_ci_auc(y_test, p_cat,   n=1000, alpha=0.05, seed=123) # Using y_test from the combined code\n",
        "ci_stack = bootstrap_ci_auc(y_test, p_stack, n=1000, alpha=0.05, seed=456) # Using y_test from the combined code\n",
        "\n",
        "def ci_to_df(ci, name):\n",
        "    return pd.DataFrame([\n",
        "        {\"model\": name, \"metric\":\"ROC_AUC\", \"mean\": ci[\"ROC_AUC\"][0], \"lo\": ci[\"ROC_AUC\"][1], \"hi\": ci[\"ROC_AUC\"][2], \"N\": ci[\"N\"]},\n",
        "        {\"model\": name, \"metric\":\"PR_AUC\",  \"mean\": ci[\"PR_AUC\"][0],  \"lo\": ci[\"PR_AUC\"][1], \"hi\": ci[\"PR_AUC\"][2], \"N\": ci[\"N\"]},\n",
        "    ])\n",
        "\n",
        "ci_df = pd.concat([ci_to_df(ci_cat,\"CatBoost\"), ci_to_df(ci_stack,\"Stacking\")], ignore_index=True)\n",
        "print(\"== 95% Bootstrap CIs for AUCs ==\")\n",
        "display(ci_df)\n",
        "\n",
        "# -------------------\n",
        "# 19) Save artifacts (tables + CSV of predictions)\n",
        "# -------------------\n",
        "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
        "pd.DataFrame({\n",
        "    \"y_true\": y_test, # Using y_test from the combined code\n",
        "    \"p_cat\": p_cat,\n",
        "    \"p_stack\": p_stack\n",
        "}).to_csv(OUT/\"test_preds_cat_vs_stack.csv\", index=False)\n",
        "\n",
        "summary_df.to_csv(OUT/\"summary_at_business_threshold.csv\", index=False)\n",
        "sweep_cat.to_csv(OUT/\"sweep_cat.csv\", index=False)\n",
        "sweep_stack.to_csv(OUT/\"sweep_stack.csv\", index=False)\n",
        "profit_all.to_csv(OUT/\"profit_curves.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" - outputs/test_preds_cat_vs_stack.csv\")\n",
        "print(\" - outputs/summary_at_business_threshold.csv\")\n",
        "print(\" - outputs/sweep_cat.csv\")\n",
        "print(\" - outputs/sweep_stack.csv\")\n",
        "print(\" - outputs/profit_curves.csv\")\n",
        "\n",
        "# -------------------\n",
        "# 20) Optional: quick plots (ROC/PR + Profit)\n",
        "# -------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_roc_pr(y_true, p1, p2, name1=\"CatBoost\", name2=\"Stacking\"):\n",
        "    fpr1, tpr1, _ = roc_curve(y_true, p1)\n",
        "    fpr2, tpr2, _ = roc_curve(y_true, p2)\n",
        "    plt.figure(); plt.plot(fpr1, tpr1, label=f\"{name1}\")\n",
        "    plt.plot(fpr2, tpr2, label=f\"{name2}\")\n",
        "    plt.plot([0,1],[0,1],\"--\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(OUT/\"roc_cat_vs_stack.png\", dpi=160); plt.close()\n",
        "\n",
        "    pr1, rc1, _ = precision_recall_curve(y_true, p1)\n",
        "    pr2, rc2, _ = precision_recall_curve(y_true, p2)\n",
        "    plt.figure(); plt.plot(rc1, pr1, label=f\"{name1}\")\n",
        "    plt.plot(rc2, pr2, label=f\"{name2}\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR Curve\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(OUT/\"pr_cat_vs_stack.png\", dpi=160); plt.close()\n",
        "\n",
        "def plot_profit(df_profit):\n",
        "    plt.figure()\n",
        "    for name, g in df_profit.groupby(\"model\"):\n",
        "        g = g.sort_values(\"threshold\")\n",
        "        plt.plot(g[\"threshold\"], g[\"profit\"], label=name)\n",
        "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Profit\"); plt.title(\"Profit Curve\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(OUT/\"profit_curves_cat_vs_stack.png\", dpi=160); plt.close()\n",
        "\n",
        "plot_roc_pr(y_test, p_cat, p_stack, \"CatBoost\", \"Stacking\") # Using y_test from the combined code\n",
        "plot_profit(profit_all)\n",
        "print(\"Saved plots:\")\n",
        "print(\" - outputs/roc_cat_vs_stack.png\")\n",
        "print(\" - outputs/pr_cat_vs_stack.png\")\n",
        "print(\" - outputs/profit_curves_cat_vs_stack.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "-4YZJf9wHCe1",
        "outputId": "78c9d87a-ad82-4d3d-e35f-a547642d2600"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "closing parenthesis ')' does not match opening parenthesis '{' on line 370 (ipython-input-2177251830.py, line 372)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2177251830.py\"\u001b[0;36m, line \u001b[0;32m372\u001b[0m\n\u001b[0;31m    \"PR_AUC\":  float(np.mean(prs)),  float(prs_ci[0]),  float(prs_ci[1])),\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '{' on line 370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Robust compare: CatBoost (single) vs Stacking\n",
        "import numpy as np, pandas as pd, json, joblib, warnings\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, f1_score,\n",
        "    precision_recall_curve, roc_curve, confusion_matrix, precision_score, recall_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ===== Safety checks =====\n",
        "CSV = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "BUNDLE = Path(\"outputs/stacking_bundle.pkl\")\n",
        "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
        "\n",
        "# Try to locate CSV if not in cwd\n",
        "if not Path(CSV).exists():\n",
        "    import os\n",
        "    def find_file_recursively(filename, roots=[\"/content\", \".\"]):\n",
        "        for root in roots:\n",
        "            for r, _, files in os.walk(root):\n",
        "                if filename in files:\n",
        "                    return os.path.join(r, filename)\n",
        "        return None\n",
        "    alt = find_file_recursively(CSV, [\"/content\", \".\"])\n",
        "    if alt:\n",
        "        print(f\"[INFO] Using CSV at: {alt}\")\n",
        "        CSV = alt\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find {CSV}. Upload it or set CSV path correctly.\"\n",
        "        )\n",
        "\n",
        "if not BUNDLE.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"outputs/stacking_bundle.pkl not found. \"\n",
        "        \"Run the training cell first (the one that saves the bundle).\"\n",
        "    )\n",
        "\n",
        "# ===== Load data & split =====\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.25\n",
        "\n",
        "df = pd.read_csv(CSV)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "if \"Churn\" not in df.columns:\n",
        "    raise ValueError(\"Column 'Churn' not found in CSV.\")\n",
        "\n",
        "y_all = df[\"Churn\"].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int).values\n",
        "X_all = df.drop(columns=[\"Churn\",\"customerID\"], errors=\"ignore\")\n",
        "\n",
        "X_tr_raw, X_te_raw, y_tr, y_te = train_test_split(\n",
        "    X_all, y_all, test_size=TEST_SIZE, stratify=y_all, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ===== Load bundle (stacking) =====\n",
        "bundle = joblib.load(BUNDLE)\n",
        "base_pipes = bundle.get(\"base_pipes\", None)\n",
        "base_order = bundle.get(\"base_order\", None)\n",
        "meta = bundle.get(\"meta\", None)\n",
        "\n",
        "if base_pipes is None or base_order is None or meta is None:\n",
        "    raise RuntimeError(\"Bundle is missing keys. Re-run training cell to regenerate the bundle.\")\n",
        "\n",
        "# Get CatBoost pipe\n",
        "if \"cat\" not in base_order:\n",
        "    raise RuntimeError(f\"'cat' not found in base_order {base_order}. \"\n",
        "                       \"Ensure the base models dict uses key 'cat'.\")\n",
        "\n",
        "cat_idx = base_order.index(\"cat\")\n",
        "pipe_cat = base_pipes[cat_idx]\n",
        "\n",
        "# ===== Predict probs =====\n",
        "p_cat = pipe_cat.predict_proba(X_te_raw)[:,1]\n",
        "test_meta = np.column_stack([p.predict_proba(X_te_raw)[:,1] for p in base_pipes])\n",
        "p_stack = meta.predict_proba(test_meta)[:,1]\n",
        "\n",
        "# ===== Threshold (profit-aware) =====\n",
        "t_star = 0.5\n",
        "mjson = Path(\"outputs/metrics.json\")\n",
        "if mjson.exists():\n",
        "    try:\n",
        "        cfg = json.loads(mjson.read_text())\n",
        "        t_star = float(cfg.get(\"best_threshold\", 0.5))\n",
        "    except Exception:\n",
        "        pass\n",
        "print(f\"[INFO] Using threshold = {t_star:.3f}\")\n",
        "\n",
        "# ===== Helpers =====\n",
        "def summarize_probs(y_true, y_score, name, thr=0.5):\n",
        "    roc = roc_auc_score(y_true, y_score)\n",
        "    pr  = average_precision_score(y_true, y_score)\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return {\n",
        "        \"model\": name, \"ROC_AUC\": roc, \"PR_AUC\": pr, \"F1@thr\": f1,\n",
        "        \"Precision@thr\": prec, \"Recall@thr\": rec, \"thr\": thr,\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn\n",
        "    }\n",
        "\n",
        "# ===== Summary @ business threshold =====\n",
        "sum_cat   = summarize_probs(y_te, p_cat,   \"CatBoost\",  t_star)\n",
        "sum_stack = summarize_probs(y_te, p_stack, \"Stacking\",  t_star)\n",
        "summary_df = pd.DataFrame([sum_cat, sum_stack])\n",
        "print(\"== Summary @ business threshold ==\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# ===== Profit curve =====\n",
        "REWARD_TP = 50.0; COST_FP = 10.0; COST_FN = 20.0\n",
        "thr_grid = np.linspace(0.05, 0.95, 37)\n",
        "\n",
        "def profit_at(y_true, y_score, thr):\n",
        "    y_pred = (y_score >= thr).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tp*REWARD_TP - fp*COST_FP - fn*COST_FN\n",
        "\n",
        "profit_cat   = pd.DataFrame({\"threshold\": thr_grid, \"profit\": [profit_at(y_te, p_cat, t) for t in thr_grid]}).assign(model=\"CatBoost\")\n",
        "profit_stack = pd.DataFrame({\"threshold\": thr_grid, \"profit\": [profit_at(y_te, p_stack, t) for t in thr_grid]}).assign(model=\"Stacking\")\n",
        "profit_all = pd.concat([profit_cat, profit_stack], ignore_index=True)\n",
        "\n",
        "best_cat   = profit_cat.loc[profit_cat[\"profit\"].idxmax()]\n",
        "best_stack = profit_stack.loc[profit_stack[\"profit\"].idxmax()]\n",
        "print(f\"Best profit CatBoost: thr={best_cat.threshold:.3f}, profit={best_cat.profit:.1f}\")\n",
        "print(f\"Best profit Stacking: thr={best_stack.threshold:.3f}, profit={best_stack.profit:.1f}\")\n",
        "\n",
        "# ===== Save outputs =====\n",
        "pd.DataFrame({\"y_true\": y_te, \"p_cat\": p_cat, \"p_stack\": p_stack}).to_csv(OUT/\"test_preds_cat_vs_stack.csv\", index=False)\n",
        "summary_df.to_csv(OUT/\"summary_at_business_threshold.csv\", index=False)\n",
        "profit_all.to_csv(OUT/\"profit_curves.csv\", index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "for f in [\"test_preds_cat_vs_stack.csv\", \"summary_at_business_threshold.csv\", \"profit_curves.csv\"]:\n",
        "    print(\" -\", OUT/f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "bF70CTq0IxLL",
        "outputId": "0ae438ab-60d1-4544-eed2-c688dc2e35ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "outputs/stacking_bundle.pkl not found. Run the training cell first (the one that saves the bundle).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2137902698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mBUNDLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;34m\"outputs/stacking_bundle.pkl not found. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"Run the training cell first (the one that saves the bundle).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: outputs/stacking_bundle.pkl not found. Run the training cell first (the one that saves the bundle)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Rebuild stacking_bundle.pkl quickly (same split & pipeline)\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost joblib\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ---- Config (match your earlier run) ----\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ---- Locate CSV (Colab or CWD) ----\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# ---- Load & basic clean ----\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found.\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# ---- Preprocessor ----\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# ---- Split (same as before) ----\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ---- Base models (slightly lighter to be fast) ----\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    return {\n",
        "        \"xgb\": XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        ),\n",
        "        \"lgb\": LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        ),\n",
        "        \"cat\": CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"rf\": RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "        \"hgb\": HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE),\n",
        "        # نقدر نسيب الـ MLP مؤقتًا لتسريع الإنقاذ، لكن هنبقيه زي ما هو:\n",
        "        # \"mlp\": MLPClassifier(hidden_layer_sizes=(128,64), activation=\"relu\", alpha=1e-3, max_iter=250, random_state=RANDOM_STATE),\n",
        "    }\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# ---- Build OOF & test meta ----\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf.__class__(**clf.get_params()))])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        test_meta[:, j] = full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ---- Meta-learner ----\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# ---- Save bundle ----\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,\n",
        "    \"meta\": meta,\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "# (اختياري) نحفظ شوية مقاييس\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_placeholder\": True\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUX6Gjk4JcgV",
        "outputId": "3c5aadc2-f30d-4add-ab21-022048dc2d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n",
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB PIPELINE: STACKING + SHAP\n",
        "# =========================================\n",
        "\n",
        "# -- Install deps (quiet) --\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "# -- Python code --\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")  # for headless savefig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters (will be used if available)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAVE_LGB = True\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    HAVE_LGB = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    HAVE_CAT = True\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    HAVE_CAT = False\n",
        "\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config & I/O\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load & basic clean\n",
        "# -------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor\n",
        "# -------------------------\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train / Test split\n",
        "# -------------------------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Base models\n",
        "# -------------------------\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    models = {}\n",
        "    if HAVE_XGB and XGBClassifier is not None:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB and LGBMClassifier is not None:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT and CatBoostClassifier is not None:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF & Test meta\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = (pipe.predict_proba(X_va_f)[:,1]\n",
        "                    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                    else pipe.decision_function(X_va_f))\n",
        "            # map scores to [0,1] if needed\n",
        "            if p_va.ndim == 1 and (p_va.min() < 0 or p_va.max() > 1):\n",
        "                from scipy.special import expit\n",
        "                p_va = expit(p_va)\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        # Fit on full train for test predictions\n",
        "        full_est = clf.__class__(**clf.get_params())\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", full_est)])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        p_test = (full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "                  if hasattr(full_pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                  else full_pipe.decision_function(X_te_idx))\n",
        "        if p_test.ndim == 1 and (p_test.min() < 0 or p_test.max() > 1):\n",
        "            from scipy.special import expit\n",
        "            p_test = expit(p_test)\n",
        "        test_meta[:, j] = p_test\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Meta-learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# quick holdout sanity\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Save bundle & metrics\n",
        "# -------------------------\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,  # full pipelines (pre + clf)\n",
        "    \"meta\": meta,\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_holdout\": {\"roc_auc\": float(meta_val_auc), \"pr_auc\": float(meta_val_pr)},\n",
        "    \"used_models\": base_names\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n",
        "# =======================================================\n",
        "# EXPLAINABILITY LAYER (SHAP)\n",
        "# 1) Meta-level (which base model drove the decision?)\n",
        "# 2) Feature-level per base model + aggregated Top-10\n",
        "# 3) Single-customer waterfall example\n",
        "# =======================================================\n",
        "\n",
        "# ---- 1) SHAP: meta-learner contributions (base learners as features) ----\n",
        "X_meta_train = oof_matrix\n",
        "X_meta_test  = test_matrix\n",
        "\n",
        "try:\n",
        "    explainer_meta = shap.LinearExplainer(meta, X_meta_train, feature_names=base_names)\n",
        "except Exception:\n",
        "    explainer_meta = shap.Explainer(meta, X_meta_train, feature_names=base_names)\n",
        "\n",
        "shap_values_meta = explainer_meta(X_meta_test)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_bar_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "meta_importance = np.mean(np.abs(shap_values_meta.values), axis=0)\n",
        "meta_importance = (pd.Series(meta_importance, index=base_names)\n",
        "                   .sort_values(ascending=False))\n",
        "meta_importance.to_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- 2) SHAP: original feature-level per base model + aggregated ----\n",
        "def get_feature_names_from_preprocessor(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "def shap_for_pipeline(pipe, X_raw_sample, max_background=512, tag=\"model\"):\n",
        "    pre = pipe.named_steps[\"pre\"]\n",
        "    clf = pipe.named_steps[\"clf\"]\n",
        "    X_enc = pre.transform(X_raw_sample)\n",
        "    feat_names = get_feature_names_from_preprocessor(pre)\n",
        "\n",
        "    # Prefer Tree/Linear explainers; fallback to Kernel for anything else\n",
        "    explainer = None\n",
        "    # Try a fast path\n",
        "    try:\n",
        "        explainer = shap.Explainer(clf, X_enc, feature_names=feat_names)\n",
        "    except Exception:\n",
        "        # KernelExplainer (slower) with a small background sample\n",
        "        bg = shap.sample(X_enc, min(max_background, X_enc.shape[0]))\n",
        "        try:\n",
        "            explainer = shap.KernelExplainer(\n",
        "                lambda data: clf.predict_proba(data)[:,1] if hasattr(clf, \"predict_proba\") else clf.decision_function(data),\n",
        "                bg\n",
        "            )\n",
        "        except Exception:\n",
        "            # final fallback: use shap.Explainer with bg\n",
        "            explainer = shap.Explainer(\n",
        "                lambda data: clf.predict_proba(data)[:,1] if hasattr(clf, \"predict_proba\") else clf.decision_function(data),\n",
        "                bg\n",
        "            )\n",
        "\n",
        "    sv = explainer(X_enc)\n",
        "\n",
        "    # Visual summaries\n",
        "    plt.figure()\n",
        "    shap.summary_plot(sv, feature_names=feat_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / f\"{tag}_summary.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    shap.summary_plot(sv, feature_names=feat_names, plot_type=\"bar\", show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / f\"{tag}_summary_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    return sv, feat_names\n",
        "\n",
        "# compact sample for speed\n",
        "X_for_explain = X_train_raw.sample(n=min(2000, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        sv, fn = shap_for_pipeline(pipe, X_for_explain, tag=f\"base_{name}\")\n",
        "        vals = sv.values\n",
        "        if isinstance(vals, list):  # sometimes SV returns list per class\n",
        "            vals = np.array(vals)\n",
        "        # collapse multiclass if exists\n",
        "        if vals.ndim == 3:\n",
        "            vals = np.mean(np.abs(vals), axis=2)\n",
        "        mean_abs = np.mean(np.abs(vals), axis=0)\n",
        "        imp = pd.Series(mean_abs, index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "        print(f\"✓ SHAP computed for base model: {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models (mean of |SHAP|)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated (all failed?).\")\n",
        "\n",
        "# ---- 3) Single-customer waterfall example (use strongest base by meta importance if possible) ----\n",
        "try:\n",
        "    # pick the base with highest meta contribution\n",
        "    best_base = meta_importance.index[0]\n",
        "    base_idx = base_names.index(best_base)\n",
        "except Exception:\n",
        "    best_base = base_names[0]\n",
        "    base_idx = 0\n",
        "\n",
        "row0 = X_test_raw.iloc[[0]]\n",
        "strong_pipe = fitted_base_pipes[base_idx]\n",
        "\n",
        "try:\n",
        "    pre = strong_pipe.named_steps[\"pre\"]\n",
        "    clf = strong_pipe.named_steps[\"clf\"]\n",
        "    X_enc = pre.transform(row0)\n",
        "    feat_names = pre.get_feature_names_out()\n",
        "    expl = shap.Explainer(clf, X_enc, feature_names=feat_names)\n",
        "    sv = expl(X_enc)\n",
        "    plt.figure()\n",
        "    shap.plots.waterfall(sv[0], show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"single_customer_waterfall.png\", dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved single customer waterfall using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer waterfall failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR} (PNG + CSVs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911,
          "referenced_widgets": [
            "84d40a658fad4850b22fe17ed960983b",
            "59206ec049ba430aadbd05bd6a8d215b",
            "70733829a689435594831fcc8fbc2bd1",
            "3aadc5625a10471b93d99bb3737122c9",
            "06c35119875d40d8a3228eef19a2ca43",
            "d8ab997abc194834b0939e5ac3558fdb",
            "3deb1a8408954e9a97581ea56684d3d2",
            "ec4f33ae62cd4a60aa513f8c57d1155c",
            "8e22f700e410461d9e4bc17acd9c6518",
            "7cb69e75960547e0a659d1e7f8c7e253",
            "6706a402d0744aa1b085419323b2d77b"
          ]
        },
        "id": "l3qHojU5mZCT",
        "outputId": "c53680b9-557d-4219-c911-a6d27b45f5e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:shap:Using 512 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved meta SHAP summaries & importances.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84d40a658fad4850b22fe17ed960983b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-404011100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitted_base_pipes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_for_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_for_explain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"base_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# sometimes SV returns list per class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-404011100.py\u001b[0m in \u001b[0;36mshap_for_pipeline\u001b[0;34m(pipe, X_raw_sample, max_background, tag)\u001b[0m\n\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0msv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;31m# Visual summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, l1_reg, silent)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_feature_names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# put outputs at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_instance_with_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mexplanations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gc_collect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# execute the model on the synthetic samples we have created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;31m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_kernel.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_index_ordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mmodelOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mmodelOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-404011100.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             explainer = shap.KernelExplainer(\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_proba\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0mclass_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1919\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1444\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_np_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m             _check_call(\n\u001b[0;32m-> 2875\u001b[0;31m                 _LIB.XGBoosterPredictFromDense(\n\u001b[0m\u001b[1;32m   2876\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m                     \u001b[0marray_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# FAST SHAP PATCH (drop-in)\n",
        "# =========================\n",
        "import shap, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "EXPLAIN_DIR = (OUT_DIR / \"explain\"); EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    name = clf.__class__.__name__.lower()\n",
        "    return any(k in name for k in [\n",
        "        \"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"\n",
        "    ])\n",
        "\n",
        "def _feature_names(pre):\n",
        "    # works with sklearn >=1.0\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# --- 2) SHAP سريع على مستوى الخصائص الأصلية لكل base model ---\n",
        "# عيّنة أصغر للتفسير (سرعة أعلى مع تمثيل كويس)\n",
        "N_SAMPLE = min(800, len(X_train_raw))\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "# نجهّز تراكيب التجميع\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "# عدد نقاط خلفية (kmeans) للخلفية\n",
        "K_BG = 64\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]\n",
        "        clf = pipe.named_steps[\"clf\"]\n",
        "\n",
        "        X_enc = pre.transform(X_for_explain)\n",
        "        X_enc = _to_dense(X_enc)  # مهم جدًا لو الـOneHot بيطلع sparse\n",
        "        feat_names = _feature_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            # المسار السريع: TreeExplainer\n",
        "            bg = shap.kmeans(X_enc, K_BG)  # خلفية مُلخّصة\n",
        "            explainer = shap.TreeExplainer(\n",
        "                clf, data=bg, model_output=\"probability\",\n",
        "                feature_perturbation=\"interventional\"\n",
        "            )\n",
        "            # إيقاف additivity check يسرّع جدًا\n",
        "            sv_values = explainer.shap_values(X_enc, check_additivity=False)\n",
        "            # اتّساق مع الإخراج الموحّد (array -> object شبيه بـ sv)\n",
        "            if isinstance(sv_values, list):  # multiclass\n",
        "                vals = np.mean([np.abs(v) for v in sv_values], axis=0)\n",
        "            else:\n",
        "                vals = np.abs(sv_values)\n",
        "        else:\n",
        "            # سريع قدر الإمكان للـlinear؛ وإلا fallback بسيط\n",
        "            try:\n",
        "                bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                explainer = shap.LinearExplainer(clf, bg, feature_names=feat_names)\n",
        "                sv = explainer(X_enc)\n",
        "                vals = np.abs(sv.values)\n",
        "            except Exception:\n",
        "                bg = shap.kmeans(X_enc, min(32, X_enc.shape[0]))\n",
        "                predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                              if hasattr(clf, \"predict_proba\")\n",
        "                              else clf.decision_function(data))\n",
        "                explainer = shap.KernelExplainer(predict_fn, bg)\n",
        "                # نقيّد بعدد محدود من التفسيرات لو لزم\n",
        "                X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                sv = explainer(X_batch)\n",
        "                vals = np.abs(sv.values)\n",
        "\n",
        "        # حساب أهمية متوسط |SHAP|\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=feat_names).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        # حفظ ملخصات مختصرة (bar فقط أسرع من scatter)\n",
        "        (imp.head(30)\n",
        "         .sort_values(ascending=True)\n",
        "         .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / f\"base_{name}_top30_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        # للتجميع العالمي\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed/slow for base model {name}: {e}\")\n",
        "\n",
        "# تجميع Across models (متوسط الأهميات)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "\n",
        "# --- 3) مثال Waterfall لعميل واحد لكن بخطوات سريعة ---\n",
        "try:\n",
        "    # اختَر أقوى base بحسب مساهمته لدى الـmeta (محسوبة قبلًا)\n",
        "    meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "    best_base = meta_imp.idxmax()\n",
        "    base_idx = base_names.index(best_base)\n",
        "except Exception:\n",
        "    best_base = base_names[0]\n",
        "    base_idx = 0\n",
        "\n",
        "row0 = X_test_raw.iloc[[0]]\n",
        "pipe = fitted_base_pipes[base_idx]\n",
        "try:\n",
        "    pre = pipe.named_steps[\"pre\"]\n",
        "    clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = pre.transform(row0)\n",
        "    X0 = _to_dense(X0)\n",
        "    fn = _feature_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        bg = shap.kmeans(_to_dense(pre.transform(X_train_raw.sample(400, random_state=RANDOM_STATE))), 32)\n",
        "        expl = shap.TreeExplainer(clf, data=bg, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "        sv_vals = expl.shap_values(X0, check_additivity=False)\n",
        "        vals0 = sv_vals if not isinstance(sv_vals, list) else np.mean([v for v in sv_vals], axis=0)\n",
        "        # ارسم waterfall يدويًا (bar sorted) أسرع من waterfall الأصلي\n",
        "        s = pd.Series(vals0[0], index=fn).abs().sort_values(ascending=False).head(15)\n",
        "        s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "        plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / \"single_customer_top15_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "    else:\n",
        "        # Linear/Kernel fallback\n",
        "        bg = shap.sample(_to_dense(pre.transform(X_train_raw.sample(400, random_state=RANDOM_STATE))), 64)\n",
        "        expl = shap.LinearExplainer(clf, bg, feature_names=fn)\n",
        "        sv = expl(X0)\n",
        "        s = pd.Series(np.abs(sv.values[0]), index=fn).sort_values(ascending=False).head(15)\n",
        "        s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "        plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / \"single_customer_top15_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SftxiDaopL-Q",
        "outputId": "502a9b5c-f579-4e6a-a21d-911e13136f08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! SHAP failed/slow for base model xgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model lgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model cat: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model rf: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model hgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! Single-customer quick chart failed: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + FAST SHAP\n",
        "# =========================================\n",
        "\n",
        "# -- Install dependencies --\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "# -- Python code (headless plotting) --\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")  # for savefig without GUI\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters (used if available)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAVE_LGB = True\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    HAVE_LGB = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    HAVE_CAT = True\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    HAVE_CAT = False\n",
        "\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config & I/O\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"   # عدّل لو الاسم مختلف\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# SHAP speed knobs\n",
        "N_SAMPLE_SHAP = 800     # عدد صفوف التفسير (كان 2000) لتسريع كبير\n",
        "K_BG = 64               # kmeans background size\n",
        "EXPLAIN_TOP_BAR = 30    # عدد الخصائص في الرسم الأفقي\n",
        "SINGLE_LOCAL_TOP = 15   # عدد الخصائص في الشارت المحلي لعميل واحد\n",
        "\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it to Colab Files or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load & basic clean\n",
        "# -------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor\n",
        "# -------------------------\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train / Test split\n",
        "# -------------------------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Base models\n",
        "# -------------------------\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    models = {}\n",
        "    if HAVE_XGB and XGBClassifier is not None:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB and LGBMClassifier is not None:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT and CatBoostClassifier is not None:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF & Test meta\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = (pipe.predict_proba(X_va_f)[:,1]\n",
        "                    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                    else pipe.decision_function(X_va_f))\n",
        "            # Map scores to [0,1] if needed\n",
        "            if p_va.ndim == 1 and (p_va.min() < 0 or p_va.max() > 1):\n",
        "                from scipy.special import expit\n",
        "                p_va = expit(p_va)\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        # Fit on full train for test predictions\n",
        "        full_est = clf.__class__(**clf.get_params())\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", full_est)])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        p_test = (full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "                  if hasattr(full_pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                  else full_pipe.decision_function(X_te_idx))\n",
        "        if p_test.ndim == 1 and (p_test.min() < 0 or p_test.max() > 1):\n",
        "            from scipy.special import expit\n",
        "            p_test = expit(p_test)\n",
        "        test_meta[:, j] = p_test\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Meta-learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# quick holdout sanity\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Save bundle & metrics\n",
        "# -------------------------\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,  # full pipelines (pre + clf)\n",
        "    \"meta\": meta,\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_holdout\": {\"roc_auc\": float(meta_val_auc), \"pr_auc\": float(meta_val_pr)},\n",
        "    \"used_models\": base_names\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n",
        "# =======================================================\n",
        "# EXPLAINABILITY (FAST SHAP)\n",
        "# 1) Meta-level (base learners contributions)\n",
        "# 2) Feature-level per base model + aggregated Top-10\n",
        "# 3) Single-customer quick local chart\n",
        "# =======================================================\n",
        "\n",
        "# ---- 1) SHAP: meta-learner contributions (base learners as features) ----\n",
        "X_meta_train = oof_matrix\n",
        "X_meta_test  = test_matrix\n",
        "\n",
        "try:\n",
        "    explainer_meta = shap.LinearExplainer(meta, X_meta_train, feature_names=base_names)\n",
        "except Exception:\n",
        "    explainer_meta = shap.Explainer(meta, X_meta_train, feature_names=base_names)\n",
        "\n",
        "shap_values_meta = explainer_meta(X_meta_test)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_bar_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "meta_importance = np.mean(np.abs(shap_values_meta.values), axis=0)\n",
        "meta_importance = (pd.Series(meta_importance, index=base_names)\n",
        "                   .sort_values(ascending=False))\n",
        "meta_importance.to_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- Helpers for fast SHAP over pipelines ----\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    name = clf.__class__.__name__.lower()\n",
        "    return any(k in name for k in [\n",
        "        \"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"\n",
        "    ])\n",
        "\n",
        "def _feature_names(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# ---- 2) Fast SHAP on original features per base model + aggregation ----\n",
        "N_SAMPLE = min(N_SAMPLE_SHAP, len(X_train_raw))\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]\n",
        "        clf = pipe.named_steps[\"clf\"]\n",
        "\n",
        "        X_enc = pre.transform(X_for_explain)\n",
        "        X_enc = _to_dense(X_enc)\n",
        "        feat_names = _feature_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            # Fast path: TreeExplainer with kmeans background\n",
        "            bg = shap.kmeans(X_enc, K_BG)\n",
        "            explainer = shap.TreeExplainer(\n",
        "                clf, data=bg, model_output=\"probability\",\n",
        "                feature_perturbation=\"interventional\"\n",
        "            )\n",
        "            sv_values = explainer.shap_values(X_enc, check_additivity=False)\n",
        "            if isinstance(sv_values, list):  # multiclass -> average abs across classes\n",
        "                vals = np.mean([np.abs(v) for v in sv_values], axis=0)\n",
        "            else:\n",
        "                vals = np.abs(sv_values)\n",
        "        else:\n",
        "            # Linear/fallback\n",
        "            try:\n",
        "                bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                explainer = shap.LinearExplainer(clf, bg, feature_names=feat_names)\n",
        "                sv = explainer(X_enc)\n",
        "                vals = np.abs(sv.values)\n",
        "            except Exception:\n",
        "                bg = shap.kmeans(X_enc, min(32, X_enc.shape[0]))\n",
        "                predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                              if hasattr(clf, \"predict_proba\")\n",
        "                              else clf.decision_function(data))\n",
        "                explainer = shap.KernelExplainer(predict_fn, bg)\n",
        "                X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                sv = explainer(X_batch)\n",
        "                vals = np.abs(sv.values)\n",
        "\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=feat_names).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        # Save compact bar chart (faster than scatter summary)\n",
        "        (imp.head(EXPLAIN_TOP_BAR)\n",
        "         .sort_values(ascending=True)\n",
        "         .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / f\"base_{name}_top{EXPLAIN_TOP_BAR}_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed/slow for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models (mean of |SHAP|)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- 3) Single-customer quick local chart ----\n",
        "try:\n",
        "    # pick base with highest meta contribution\n",
        "    try:\n",
        "        meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "        best_base = meta_imp.idxmax()\n",
        "        base_idx = base_names.index(best_base)\n",
        "    except Exception:\n",
        "        best_base = base_names[0]\n",
        "        base_idx = 0\n",
        "\n",
        "    row0 = X_test_raw.iloc[[0]]\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre = pipe.named_steps[\"pre\"]\n",
        "    clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = pre.transform(row0)\n",
        "    X0 = _to_dense(X0)\n",
        "    fn = _feature_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        bg = shap.kmeans(_to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE))), 32)\n",
        "        expl = shap.TreeExplainer(clf, data=bg, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "        sv_vals = explainer.shap_values(X0, check_additivity=False) if 'explainer' in locals() else expl.shap_values(X0, check_additivity=False)\n",
        "        vals0 = sv_vals if not isinstance(sv_vals, list) else np.mean([v for v in sv_vals], axis=0)\n",
        "        s = pd.Series(np.abs(vals0[0]), index=fn).sort_values(ascending=False).head(SINGLE_LOCAL_TOP)\n",
        "    else:\n",
        "        bg = shap.sample(_to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE))), 64)\n",
        "        expl = shap.LinearExplainer(clf, bg, feature_names=fn)\n",
        "        sv = expl(X0)\n",
        "        s = pd.Series(np.abs(sv.values[0]), index=fn).sort_values(ascending=False).head(SINGLE_LOCAL_TOP)\n",
        "\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"single_customer_top_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR} (PNGs + CSVs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMY5dkjqHkP",
        "outputId": "d3774e65-eb0b-413b-9d6b-f3ca78f1972a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n",
            "✓ Saved meta SHAP summaries & importances.\n",
            "! SHAP failed/slow for base model xgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model lgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model cat: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model rf: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed/slow for base model hgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! No per-model SHAP importances aggregated.\n",
            "! Single-customer quick chart failed: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain (PNGs + CSVs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + FAST SHAP (maskers)\n",
        "# Compatible with newer SHAP (>=0.45)\n",
        "# =========================================\n",
        "\n",
        "# -- Install dependencies --\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "# -- Python code (headless plotting) --\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")  # for savefig without GUI\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters (used if available)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAVE_LGB = True\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    HAVE_LGB = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    HAVE_CAT = True\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    HAVE_CAT = False\n",
        "\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config & I/O\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"   # عدّل لو الاسم مختلف\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# SHAP speed knobs\n",
        "N_SAMPLE_SHAP = 800     # عدد صفوف التفسير (كان 2000) لتسريع كبير\n",
        "K_BG = 64               # kmeans/Independent background size\n",
        "EXPLAIN_TOP_BAR = 30    # عدد الخصائص في الرسم الأفقي\n",
        "SINGLE_LOCAL_TOP = 15   # عدد الخصائص في الشارت المحلي لعميل واحد\n",
        "\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it to Colab Files or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load & basic clean\n",
        "# -------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor\n",
        "# -------------------------\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train / Test split\n",
        "# -------------------------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Base models\n",
        "# -------------------------\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    models = {}\n",
        "    if HAVE_XGB and XGBClassifier is not None:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB and LGBMClassifier is not None:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT and CatBoostClassifier is not None:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF & Test meta\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = (pipe.predict_proba(X_va_f)[:,1]\n",
        "                    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                    else pipe.decision_function(X_va_f))\n",
        "            # Map scores to [0,1] if needed\n",
        "            if p_va.ndim == 1 and (p_va.min() < 0 or p_va.max() > 1):\n",
        "                from scipy.special import expit\n",
        "                p_va = expit(p_va)\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        # Fit on full train for test predictions\n",
        "        full_est = clf.__class__(**clf.get_params())\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", full_est)])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        p_test = (full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "                  if hasattr(full_pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                  else full_pipe.decision_function(X_te_idx))\n",
        "        if p_test.ndim == 1 and (p_test.min() < 0 or p_test.max() > 1):\n",
        "            from scipy.special import expit\n",
        "            p_test = expit(p_test)\n",
        "        test_meta[:, j] = p_test\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Meta-learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "# quick holdout sanity\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Save bundle & metrics\n",
        "# -------------------------\n",
        "bundle = {\n",
        "    \"base_pipes\": fitted_base_pipes,  # full pipelines (pre + clf)\n",
        "    \"meta\": meta,\n",
        "    \"base_order\": base_names\n",
        "}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "artifacts = {\n",
        "    \"base_oof_scores\": base_oof_scores,\n",
        "    \"meta_holdout\": {\"roc_auc\": float(meta_val_auc), \"pr_auc\": float(meta_val_pr)},\n",
        "    \"used_models\": base_names\n",
        "}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n",
        "# =======================================================\n",
        "# EXPLAINABILITY (FAST SHAP with maskers)\n",
        "# 1) Meta-level (base learners contributions)\n",
        "# 2) Feature-level per base model + aggregated Top-10\n",
        "# 3) Single-customer quick local chart\n",
        "# =======================================================\n",
        "\n",
        "# ---- Helpers ----\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    name = clf.__class__.__name__.lower()\n",
        "    return any(k in name for k in [\n",
        "        \"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"\n",
        "    ])\n",
        "\n",
        "def _feature_names(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# ---- 1) SHAP: meta-learner contributions (base learners as features) ----\n",
        "X_meta_train = oof_matrix\n",
        "X_meta_test  = test_matrix\n",
        "\n",
        "try:\n",
        "    masker_meta = shap.maskers.Independent(X_meta_train)\n",
        "    explainer_meta = shap.LinearExplainer(meta, masker_meta, feature_names=base_names)\n",
        "except Exception:\n",
        "    explainer_meta = shap.Explainer(meta, shap.maskers.Independent(X_meta_train), feature_names=base_names)\n",
        "\n",
        "shap_values_meta = explainer_meta(X_meta_test)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_bar_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "meta_importance = np.mean(np.abs(shap_values_meta.values), axis=0)\n",
        "meta_importance = (pd.Series(meta_importance, index=base_names)\n",
        "                   .sort_values(ascending=False))\n",
        "meta_importance.to_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- 2) Fast SHAP on original features per base model + aggregation (with maskers) ----\n",
        "N_SAMPLE = min(N_SAMPLE_SHAP, len(X_train_raw))\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]\n",
        "        clf = pipe.named_steps[\"clf\"]\n",
        "\n",
        "        X_enc = pre.transform(X_for_explain)\n",
        "        X_enc = _to_dense(X_enc)\n",
        "        feat_names = _feature_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            # Fast path: TreeExplainer with Independent masker (bg from kmeans/sample)\n",
        "            # استخدم kmeans لو تحب؛ لكن Independent + sample سريع ومقبول\n",
        "            bg = shap.sample(X_enc, min(K_BG*10, X_enc.shape[0]))\n",
        "            masker = shap.maskers.Independent(bg)\n",
        "            explainer = shap.TreeExplainer(\n",
        "                clf, masker=masker, model_output=\"probability\",\n",
        "                feature_perturbation=\"interventional\"\n",
        "            )\n",
        "            sv = explainer(X_enc, check_additivity=False)\n",
        "            vals = np.abs(sv.values)  # (n_samples, n_features) أو (n_samples, n_classes, n_features)\n",
        "            if vals.ndim == 3:        # multiclass → متوسط |SHAP| عبر الكلاسات\n",
        "                vals = np.mean(vals, axis=1)\n",
        "        else:\n",
        "            # Linear/fallback with Independent masker\n",
        "            bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "            masker = shap.maskers.Independent(bg)\n",
        "            try:\n",
        "                explainer = shap.LinearExplainer(clf, masker, feature_names=feat_names)\n",
        "                sv = explainer(X_enc)\n",
        "                vals = np.abs(sv.values)\n",
        "            except Exception:\n",
        "                # Kernel fallback (أبطأ؛ نقيد الحجم)\n",
        "                predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                              if hasattr(clf, \"predict_proba\")\n",
        "                              else clf.decision_function(data))\n",
        "                explainer = shap.KernelExplainer(predict_fn, bg)\n",
        "                X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                sv = explainer(X_batch)\n",
        "                vals = np.abs(sv.values)\n",
        "\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=feat_names).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        # Save compact bar chart (faster than scatter summary)\n",
        "        (imp.head(EXPLAIN_TOP_BAR)\n",
        "         .sort_values(ascending=True)\n",
        "         .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / f\"base_{name}_top{EXPLAIN_TOP_BAR}_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models (mean of |SHAP|)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- 3) Single-customer quick local chart (with masker) ----\n",
        "try:\n",
        "    # pick base with highest meta contribution\n",
        "    try:\n",
        "        meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "        best_base = meta_imp.idxmax()\n",
        "        base_idx = base_names.index(best_base)\n",
        "    except Exception:\n",
        "        best_base = base_names[0]\n",
        "        base_idx = 0\n",
        "\n",
        "    row0 = X_test_raw.iloc[[0]]\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre = pipe.named_steps[\"pre\"]\n",
        "    clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = _to_dense(pre.transform(row0))\n",
        "    fn = _feature_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "        masker = shap.maskers.Independent(bg)\n",
        "        expl = shap.TreeExplainer(clf, masker=masker, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "        sv0 = expl(X0, check_additivity=False)\n",
        "        vals0 = np.abs(sv0.values[0])\n",
        "        if vals0.ndim == 2:  # multiclass (n_classes, n_features)\n",
        "            vals0 = np.mean(vals0, axis=0)\n",
        "    else:\n",
        "        bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "        masker = shap.maskers.Independent(bg)\n",
        "        expl = shap.LinearExplainer(clf, masker, feature_names=fn)\n",
        "        sv0 = expl(X0)\n",
        "        vals0 = np.abs(sv0.values[0])\n",
        "\n",
        "    s = pd.Series(vals0, index=fn).sort_values(ascending=False).head(SINGLE_LOCAL_TOP)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"single_customer_top_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR} (PNGs + CSVs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWVhUAJerMgU",
        "outputId": "1b2a9c93-469f-4654-82ae-2a68a79ab1e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n",
            "✓ Saved meta SHAP summaries & importances.\n",
            "! SHAP failed for base model xgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model lgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model cat: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model rf: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model hgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! No per-model SHAP importances aggregated.\n",
            "! Single-customer quick chart failed: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain (PNGs + CSVs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + FAST SHAP\n",
        "# Version-robust for SHAP (old/new APIs)\n",
        "# =========================================\n",
        "\n",
        "# -- Install deps --\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib packaging\n",
        "\n",
        "# -- Python code --\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "from packaging.version import Version\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAVE_LGB = True\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    HAVE_LGB = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    HAVE_CAT = True\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    HAVE_CAT = False\n",
        "\n",
        "import shap\n",
        "SHAP_VER = Version(shap.__version__)\n",
        "print(\"SHAP version:\", SHAP_VER)\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# SHAP knobs\n",
        "N_SAMPLE_SHAP = 800   # speed-friendly sample size\n",
        "K_BG = 64             # background size (kmeans/sample)\n",
        "EXPLAIN_TOP_BAR = 30\n",
        "SINGLE_LOCAL_TOP = 15\n",
        "\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load & clean\n",
        "# -------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor\n",
        "# -------------------------\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Split\n",
        "# -------------------------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Base models\n",
        "# -------------------------\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    models = {}\n",
        "    if HAVE_XGB and XGBClassifier is not None:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB and LGBMClassifier is not None:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT and CatBoostClassifier is not None:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# OOF & Test matrix\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = (pipe.predict_proba(X_va_f)[:,1]\n",
        "                    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                    else pipe.decision_function(X_va_f))\n",
        "            if p_va.ndim == 1 and (p_va.min() < 0 or p_va.max() > 1):\n",
        "                from scipy.special import expit\n",
        "                p_va = expit(p_va)\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        full_est = clf.__class__(**clf.get_params())\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", full_est)])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        p_test = (full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "                  if hasattr(full_pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                  else full_pipe.decision_function(X_te_idx))\n",
        "        if p_test.ndim == 1 and (p_test.min() < 0 or p_test.max() > 1):\n",
        "            from scipy.special import expit\n",
        "            p_test = expit(p_test)\n",
        "        test_meta[:, j] = p_test\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Meta-learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "# Save bundle & metrics\n",
        "bundle = {\"base_pipes\": fitted_base_pipes, \"meta\": meta, \"base_order\": base_names}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "artifacts = {\"base_oof_scores\": base_oof_scores, \"meta_holdout\": {\"roc_auc\": float(meta_val_auc), \"pr_auc\": float(meta_val_pr)}, \"used_models\": base_names}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n",
        "# =======================================================\n",
        "# EXPLAINABILITY (FAST SHAP) — version-robust\n",
        "# =======================================================\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    n = clf.__class__.__name__.lower()\n",
        "    return any(k in n for k in [\"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"])\n",
        "\n",
        "def _feat_names(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# ---- (1) Meta-level SHAP (base learners as features) ----\n",
        "X_meta_train, X_meta_test = oof_matrix, test_matrix\n",
        "try:\n",
        "    if SHAP_VER >= Version(\"0.45.0\"):\n",
        "        masker_meta = shap.maskers.Independent(X_meta_train)\n",
        "        explainer_meta = shap.LinearExplainer(meta, masker_meta, feature_names=base_names)\n",
        "        sv_meta = explainer_meta(X_meta_test)\n",
        "    else:\n",
        "        explainer_meta = shap.LinearExplainer(meta, X_meta_train)\n",
        "        sv_meta = explainer_meta.shap_values(X_meta_test)  # old API returns np.ndarray\n",
        "        # Wrap to unified object-like structure\n",
        "        class _SV:\n",
        "            def __init__(self, values): self.values = values\n",
        "        sv_meta = _SV(sv_meta)\n",
        "except Exception:\n",
        "    explainer_meta = shap.Explainer(meta, X_meta_train, feature_names=base_names)\n",
        "    sv_meta = explainer_meta(X_meta_test)\n",
        "\n",
        "plt.figure(); shap.summary_plot(sv_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_base_contributions.png\", dpi=200); plt.close()\n",
        "plt.figure(); shap.summary_plot(sv_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_bar_base_contributions.png\", dpi=200); plt.close()\n",
        "\n",
        "meta_imp = np.mean(np.abs(getattr(sv_meta, \"values\", sv_meta)), axis=0)\n",
        "meta_imp = pd.Series(meta_imp, index=base_names).sort_values(ascending=False)\n",
        "meta_imp.to_csv(EXPLAIN_DIR/\"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- (2) Per-base model SHAP on original features + aggregation ----\n",
        "N_SAMPLE = min(N_SAMPLE_SHAP, len(X_train_raw))\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "        X_enc = _to_dense(pre.transform(X_for_explain))\n",
        "        fn = _feat_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            if SHAP_VER >= Version(\"0.45.0\"):\n",
        "                bg = shap.sample(X_enc, min(K_BG*10, X_enc.shape[0]))\n",
        "                masker = shap.maskers.Independent(bg)\n",
        "                expl = shap.TreeExplainer(clf, masker=masker, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "                sv = expl(X_enc, check_additivity=False)\n",
        "                vals = np.abs(sv.values)\n",
        "                if vals.ndim == 3:  # (n_samples, n_classes, n_features)\n",
        "                    vals = np.mean(vals, axis=1)\n",
        "            else:\n",
        "                # old API\n",
        "                try:\n",
        "                    bg = shap.kmeans(X_enc, K_BG)\n",
        "                except Exception:\n",
        "                    bg = shap.sample(X_enc, min(K_BG*10, X_enc.shape[0]))\n",
        "                expl = shap.TreeExplainer(clf, data=bg, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "                sv_vals = expl.shap_values(X_enc, check_additivity=False)\n",
        "                if isinstance(sv_vals, list):  # multiclass\n",
        "                    vals = np.mean([np.abs(v) for v in sv_vals], axis=0)\n",
        "                else:\n",
        "                    vals = np.abs(sv_vals)\n",
        "        else:\n",
        "            # Linear / others\n",
        "            if SHAP_VER >= Version(\"0.45.0\"):\n",
        "                bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                masker = shap.maskers.Independent(bg)\n",
        "                expl = shap.LinearExplainer(clf, masker, feature_names=fn)\n",
        "                sv = expl(X_enc)\n",
        "                vals = np.abs(sv.values)\n",
        "            else:\n",
        "                try:\n",
        "                    bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                    expl = shap.LinearExplainer(clf, bg)\n",
        "                    sv_vals = expl.shap_values(X_enc)\n",
        "                    vals = np.abs(sv_vals)\n",
        "                except Exception:\n",
        "                    # Kernel fallback (restrict size)\n",
        "                    predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                                  if hasattr(clf, \"predict_proba\")\n",
        "                                  else clf.decision_function(data))\n",
        "                    try:\n",
        "                        bg = shap.kmeans(X_enc, min(32, X_enc.shape[0]))\n",
        "                    except Exception:\n",
        "                        bg = shap.sample(X_enc, min(256, X_enc.shape[0]))\n",
        "                    expl = shap.KernelExplainer(predict_fn, bg)\n",
        "                    X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                    sv_vals = expl.shap_values(X_batch)\n",
        "                    vals = np.abs(sv_vals)\n",
        "\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        (imp.head(EXPLAIN_TOP_BAR).sort_values(ascending=True)\n",
        "            .plot(kind=\"barh\", figsize=(6,8)))\n",
        "        plt.tight_layout(); plt.savefig(EXPLAIN_DIR/f\"base_{name}_top{EXPLAIN_TOP_BAR}_bar.png\", dpi=200); plt.close()\n",
        "\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)).to_csv(EXPLAIN_DIR/\"per_model_top10.csv\")\n",
        "    agg.to_csv(EXPLAIN_DIR/\"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR/\"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5,4)))\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"global_top10_bar.png\", dpi=200); plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- (3) Single-customer quick local chart (version-robust) ----\n",
        "try:\n",
        "    try:\n",
        "        meta_imp = pd.read_csv(EXPLAIN_DIR/\"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "        best_base = meta_imp.idxmax()\n",
        "        base_idx = base_names.index(best_base)\n",
        "    except Exception:\n",
        "        best_base, base_idx = base_names[0], 0\n",
        "\n",
        "    row0 = X_test_raw.iloc[[0]]\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = _to_dense(pre.transform(row0)); fn = _feat_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        if SHAP_VER >= Version(\"0.45.0\"):\n",
        "            bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "            masker = shap.maskers.Independent(bg)\n",
        "            expl = shap.TreeExplainer(clf, masker=masker, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "            sv0 = expl(X0, check_additivity=False)\n",
        "            vals0 = np.abs(sv0.values[0])\n",
        "            if vals0.ndim == 2:  # (n_classes, n_features)\n",
        "                vals0 = np.mean(vals0, axis=0)\n",
        "        else:\n",
        "            try:\n",
        "                bg = shap.kmeans(_to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE))), 32)\n",
        "            except Exception:\n",
        "                bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "            expl = shap.TreeExplainer(clf, data=bg, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "            sv_vals = expl.shap_values(X0, check_additivity=False)\n",
        "            if isinstance(sv_vals, list):\n",
        "                vals0 = np.mean([np.array(v)[0] for v in sv_vals], axis=0)\n",
        "            else:\n",
        "                vals0 = np.abs(np.array(sv_vals)[0])\n",
        "    else:\n",
        "        if SHAP_VER >= Version(\"0.45.0\"):\n",
        "            bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "            masker = shap.maskers.Independent(bg)\n",
        "            expl = shap.LinearExplainer(clf, masker, feature_names=fn)\n",
        "            sv = expl(X0)\n",
        "            vals0 = np.abs(sv.values[0])\n",
        "        else:\n",
        "            bg = _to_dense(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)))\n",
        "            expl = shap.LinearExplainer(clf, bg)\n",
        "            sv_vals = expl.shap_values(X0)\n",
        "            vals0 = np.abs(np.array(sv_vals)[0])\n",
        "\n",
        "    s = pd.Series(vals0, index=fn).sort_values(ascending=False).head(SINGLE_LOCAL_TOP)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"single_customer_top_bar.png\", dpi=200); plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR} (PNGs + CSVs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh_O5uZssccg",
        "outputId": "7365ebb4-e649-470f-86b9-c410cddaca68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAP version: 0.49.1\n",
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n",
            "✓ Saved meta SHAP summaries & importances.\n",
            "! SHAP failed for base model xgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model lgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model cat: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model rf: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! SHAP failed for base model hgb: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "! No per-model SHAP importances aggregated.\n",
            "! Single-customer quick chart failed: TreeExplainer.__init__() got an unexpected keyword argument 'masker'\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain (PNGs + CSVs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# EXPLAINABILITY (FAST SHAP) — TreeExplainer with data=\n",
        "# Compatible with SHAP 0.49.1 (no `masker` kw)\n",
        "# =======================================================\n",
        "\n",
        "import shap, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    n = clf.__class__.__name__.lower()\n",
        "    return any(k in n for k in [\"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"])\n",
        "\n",
        "def _feat_names(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# ---- (A) Per-base model SHAP على الخصائص الأصلية + تجميع ----\n",
        "N_SAMPLE = min(800, len(X_train_raw))   # تقدر تزودها لو حابب الدقة أعلى\n",
        "K_BG = 64                               # حجم الخلفية لـ kmeans\n",
        "EXPLAIN_TOP_BAR = 30\n",
        "\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "        X_enc = _to_dense(pre.transform(X_for_explain))\n",
        "        fn = _feat_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            # المسار السريع للموديلات الشجرية: TreeExplainer بـ data=\n",
        "            try:\n",
        "                bg = shap.kmeans(X_enc, K_BG)\n",
        "            except Exception:\n",
        "                bg = shap.sample(X_enc, min(K_BG*10, X_enc.shape[0]))\n",
        "\n",
        "            expl = shap.TreeExplainer(\n",
        "                clf,\n",
        "                data=bg,                            # << المفتاح هنا\n",
        "                model_output=\"probability\",\n",
        "                feature_perturbation=\"interventional\"\n",
        "            )\n",
        "            sv_vals = expl.shap_values(X_enc, check_additivity=False)\n",
        "\n",
        "            # توحيد الشكل: قد تكون list في حالات multiclass\n",
        "            if isinstance(sv_vals, list):\n",
        "                vals = np.mean([np.abs(v) for v in sv_vals], axis=0)  # متوسط |SHAP| عبر الكلاسات\n",
        "            else:\n",
        "                vals = np.abs(sv_vals)\n",
        "        else:\n",
        "            # Linear/fallback: LinearExplainer بـ data= أو KernelExplainer\n",
        "            try:\n",
        "                bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                expl = shap.LinearExplainer(clf, bg)\n",
        "                vals = np.abs(expl.shap_values(X_enc))\n",
        "            except Exception:\n",
        "                # Kernel fallback (أبطأ؛ نقيد الحجم)\n",
        "                try:\n",
        "                    bg = shap.kmeans(X_enc, min(32, X_enc.shape[0]))\n",
        "                except Exception:\n",
        "                    bg = shap.sample(X_enc, min(256, X_enc.shape[0]))\n",
        "                predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                              if hasattr(clf, \"predict_proba\")\n",
        "                              else clf.decision_function(data))\n",
        "                expl = shap.KernelExplainer(predict_fn, bg)\n",
        "                X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                vals = np.abs(expl.shap_values(X_batch))\n",
        "\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        # رسم سريع (bar) بدلاً من scatter\n",
        "        (imp.head(EXPLAIN_TOP_BAR).sort_values(ascending=True)\n",
        "            .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / f\"base_{name}_top{EXPLAIN_TOP_BAR}_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# تجميع عبر الموديلات (متوسط |SHAP|)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- (B) Single-customer quick local chart (باستخدام data=) ----\n",
        "try:\n",
        "    try:\n",
        "        meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "        best_base = meta_imp.idxmax()\n",
        "        base_idx = base_names.index(best_base)\n",
        "    except Exception:\n",
        "        best_base, base_idx = base_names[0], 0\n",
        "\n",
        "    row0 = X_test_raw.iloc[[0]]\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = _to_dense(pre.transform(row0)); fn = _feat_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        try:\n",
        "            bg0 = shap.kmeans(_to_dense(pre.transform(\n",
        "                X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "            )), 32)\n",
        "        except Exception:\n",
        "            bg0 = _to_dense(pre.transform(\n",
        "                X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "            ))\n",
        "\n",
        "        expl0 = shap.TreeExplainer(\n",
        "            clf,\n",
        "            data=bg0,                          # << المهم\n",
        "            model_output=\"probability\",\n",
        "            feature_perturbation=\"interventional\"\n",
        "        )\n",
        "        sv0 = expl0.shap_values(X0, check_additivity=False)\n",
        "        if isinstance(sv0, list):\n",
        "            vals0 = np.mean([np.array(v)[0] for v in sv0], axis=0)\n",
        "        else:\n",
        "            vals0 = np.abs(np.array(sv0)[0])\n",
        "    else:\n",
        "        bg0 = _to_dense(pre.transform(\n",
        "            X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "        ))\n",
        "        expl0 = shap.LinearExplainer(clf, bg0)\n",
        "        vals0 = np.abs(np.array(expl0.shap_values(X0))[0])\n",
        "\n",
        "    s = pd.Series(vals0, index=fn).sort_values(ascending=False).head(15)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"single_customer_top_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3gVXGTotXFI",
        "outputId": "9182dbea-2cad-4758-f6c5-31fb19254984"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! SHAP failed for base model xgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed for base model lgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed for base model cat: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed for base model rf: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! SHAP failed for base model hgb: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n",
            "! No per-model SHAP importances aggregated.\n",
            "! Single-customer quick chart failed: Unsupported masker type: <class 'shap.utils._legacy.DenseData'>!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + SHAP (robust)\n",
        "# Works with SHAP 0.49.x (no `masker` kw)\n",
        "# =========================================\n",
        "\n",
        "# -- Install deps --\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "# -- Python code --\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")  # headless plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "    HAVE_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAVE_LGB = True\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    HAVE_LGB = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    HAVE_CAT = True\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    HAVE_CAT = False\n",
        "\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"   # غيّر لو الاسم مختلف\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# SHAP speed knobs\n",
        "N_SAMPLE_SHAP = 800     # حجم العينة للتفسير (سرعة مقابل دقة)\n",
        "K_BG = 64               # حجم الخلفية (kmeans/sample)\n",
        "EXPLAIN_TOP_BAR = 30    # عدد الخصائص في الـbar chart\n",
        "SINGLE_LOCAL_TOP = 15   # عدد الخصائص المحلية (عميل واحد)\n",
        "\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_file_recursively(filename: str, roots: List[str] = [\"/content\", \".\"]) -> str:\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"Could not find {filename}. Upload it to Colab Files or set CSV_FILE_NAME.\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME, [\"/content\", \".\"])\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load & clean\n",
        "# -------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns:\n",
        "    df = df.drop(columns=[\"customerID\"])\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor\n",
        "# -------------------------\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Split\n",
        "# -------------------------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Base models\n",
        "# -------------------------\n",
        "def get_base_models() -> Dict[str, object]:\n",
        "    models = {}\n",
        "    if HAVE_XGB and XGBClassifier is not None:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB and LGBMClassifier is not None:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT and CatBoostClassifier is not None:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# OOF & Test matrix\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(\n",
        "    models: Dict[str, object],\n",
        "    X_tr_raw: pd.DataFrame, y_tr: np.ndarray,\n",
        "    X_te_raw: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    n_train = X_tr_raw.shape[0]\n",
        "    n_test  = X_te_raw.shape[0]\n",
        "    oof = np.zeros((n_train, len(models)), dtype=float)\n",
        "    test_meta = np.zeros((n_test, len(models)), dtype=float)\n",
        "    per_model_scores = {}\n",
        "    fitted_full_pipes = []\n",
        "\n",
        "    X_tr_idx = X_tr_raw.reset_index(drop=True)\n",
        "    X_te_idx = X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j, (name, clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train, dtype=float)\n",
        "        fold_scores = []\n",
        "\n",
        "        for tr_idx, va_idx in skf.split(X_tr_idx, y_tr_idx):\n",
        "            X_tr_f, X_va_f = X_tr_idx.iloc[tr_idx], X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f, y_va_f = y_tr_idx[tr_idx], y_tr_idx[va_idx]\n",
        "\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "            pipe.fit(X_tr_f, y_tr_f)\n",
        "            p_va = (pipe.predict_proba(X_va_f)[:,1]\n",
        "                    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                    else pipe.decision_function(X_va_f))\n",
        "            if p_va.ndim == 1 and (p_va.min() < 0 or p_va.max() > 1):\n",
        "                from scipy.special import expit\n",
        "                p_va = expit(p_va)\n",
        "            preds_oof[va_idx] = p_va\n",
        "            fold_scores.append((\n",
        "                roc_auc_score(y_va_f, p_va),\n",
        "                average_precision_score(y_va_f, p_va)\n",
        "            ))\n",
        "\n",
        "        aucs = [a for a, _ in fold_scores]\n",
        "        prs  = [p for _, p in fold_scores]\n",
        "        per_model_scores[name] = {\n",
        "            \"oof_roc_auc_mean\": float(np.mean(aucs)),\n",
        "            \"oof_pr_auc_mean\":  float(np.mean(prs))\n",
        "        }\n",
        "        oof[:, j] = preds_oof\n",
        "\n",
        "        full_est = clf.__class__(**clf.get_params())\n",
        "        full_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", full_est)])\n",
        "        full_pipe.fit(X_tr_idx, y_tr_idx)\n",
        "        p_test = (full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "                  if hasattr(full_pipe.named_steps[\"clf\"], \"predict_proba\")\n",
        "                  else full_pipe.decision_function(X_te_idx))\n",
        "        if p_test.ndim == 1 and (p_test.min() < 0 or p_test.max() > 1):\n",
        "            from scipy.special import expit\n",
        "            p_test = expit(p_test)\n",
        "        test_meta[:, j] = p_test\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "\n",
        "        print(f\"[{name}] OOF ROC-AUC={per_model_scores[name]['oof_roc_auc_mean']:.4f} | PR-AUC={per_model_scores[name]['oof_pr_auc_mean']:.4f}\")\n",
        "\n",
        "    return oof, test_meta, per_model_scores, fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix, test_matrix, base_oof_scores, fitted_base_pipes = build_oof_and_test_matrix(\n",
        "    base_models, X_train_raw, y_train, X_test_raw, n_splits=N_FOLDS, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Meta-learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix, y_train)\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "# Save bundle & metrics\n",
        "bundle = {\"base_pipes\": fitted_base_pipes, \"meta\": meta, \"base_order\": base_names}\n",
        "joblib.dump(bundle, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "artifacts = {\"base_oof_scores\": base_oof_scores, \"meta_holdout\": {\"roc_auc\": float(meta_val_auc), \"pr_auc\": float(meta_val_pr)}, \"used_models\": base_names}\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps(artifacts, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"metrics.json\")\n",
        "\n",
        "# =======================================================\n",
        "# EXPLAINABILITY (SHAP) — robust for 0.49.x\n",
        "# =======================================================\n",
        "def _to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def _to_ndarray(bg):\n",
        "    \"\"\"Accept numpy/pandas or SHAP DenseData -> np.ndarray\"\"\"\n",
        "    if hasattr(bg, \"data\"):  # DenseData legacy\n",
        "        return np.asarray(bg.data)\n",
        "    return np.asarray(bg)\n",
        "\n",
        "def _is_tree_model(clf):\n",
        "    n = clf.__class__.__name__.lower()\n",
        "    return any(k in n for k in [\"xgb\", \"lgbm\", \"catboost\", \"forest\", \"gradientboosting\", \"histgradient\"])\n",
        "\n",
        "def _feat_names(pre):\n",
        "    return pre.get_feature_names_out()\n",
        "\n",
        "# ---- (1) Meta-level SHAP (base learners as features) ----\n",
        "# Use old-style LinearExplainer with data= (compatible & fast)\n",
        "explainer_meta = shap.LinearExplainer(meta, oof_matrix)\n",
        "sv_meta = explainer_meta.shap_values(test_matrix)  # ndarray shape (n_test, n_models)\n",
        "\n",
        "# Save meta-level charts\n",
        "plt.figure()\n",
        "shap.summary_plot(sv_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(sv_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPLAIN_DIR / \"meta_summary_bar_base_contributions.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "meta_importance = np.mean(np.abs(sv_meta), axis=0)\n",
        "pd.Series(meta_importance, index=base_names).sort_values(ascending=False)\\\n",
        "  .to_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- (2) Per-base model SHAP on original features + aggregation ----\n",
        "N_SAMPLE = min(N_SAMPLE_SHAP, len(X_train_raw))\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "        X_enc = _to_dense(pre.transform(X_for_explain))\n",
        "        fn = _feat_names(pre)\n",
        "\n",
        "        if _is_tree_model(clf):\n",
        "            # TreeExplainer with data= and ndarray background\n",
        "            try:\n",
        "                bg = shap.kmeans(X_enc, K_BG)\n",
        "            except Exception:\n",
        "                bg = shap.sample(X_enc, min(K_BG*10, X_enc.shape[0]))\n",
        "            bg_arr = _to_ndarray(bg)\n",
        "\n",
        "            expl = shap.TreeExplainer(\n",
        "                clf,\n",
        "                data=bg_arr,\n",
        "                model_output=\"probability\",\n",
        "                feature_perturbation=\"interventional\"\n",
        "            )\n",
        "            sv_vals = expl.shap_values(X_enc, check_additivity=False)\n",
        "\n",
        "            if isinstance(sv_vals, list):   # multiclass -> mean |SHAP| across classes\n",
        "                vals = np.mean([np.abs(v) for v in sv_vals], axis=0)\n",
        "            else:\n",
        "                vals = np.abs(sv_vals)\n",
        "        else:\n",
        "            # LinearExplainer with data=; fallback to Kernel if needed\n",
        "            try:\n",
        "                bg = shap.sample(X_enc, min(128, X_enc.shape[0]))\n",
        "                bg_arr = _to_ndarray(bg)\n",
        "                expl = shap.LinearExplainer(clf, bg_arr)\n",
        "                vals = np.abs(expl.shap_values(X_enc))\n",
        "            except Exception:\n",
        "                try:\n",
        "                    bg = shap.kmeans(X_enc, min(32, X_enc.shape[0]))\n",
        "                except Exception:\n",
        "                    bg = shap.sample(X_enc, min(256, X_enc.shape[0]))\n",
        "                bg_arr = _to_ndarray(bg)\n",
        "                predict_fn = (lambda data: clf.predict_proba(data)[:,1]\n",
        "                              if hasattr(clf, \"predict_proba\")\n",
        "                              else clf.decision_function(data))\n",
        "                expl = shap.KernelExplainer(predict_fn, bg_arr)\n",
        "                X_batch = shap.sample(X_enc, min(400, X_enc.shape[0]))\n",
        "                vals = np.abs(expl.shap_values(X_batch))\n",
        "\n",
        "        mean_abs = np.mean(vals, axis=0)\n",
        "        imp = pd.Series(mean_abs, index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        (imp.head(EXPLAIN_TOP_BAR).sort_values(ascending=True)\n",
        "            .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXPLAIN_DIR / f\"base_{name}_top{EXPLAIN_TOP_BAR}_bar.png\", dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        for f, v in imp.items():\n",
        "            global_feat_importance[f].append(v)\n",
        "\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models (mean |SHAP|)\n",
        "if len(global_feat_importance) > 0:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    (pd.DataFrame(per_model_top10).fillna(0.0)\n",
        "        .to_csv(EXPLAIN_DIR / \"per_model_top10.csv\"))\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- (3) Single-customer quick local chart ----\n",
        "try:\n",
        "    try:\n",
        "        meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "        best_base = meta_imp.idxmax(); base_idx = base_names.index(best_base)\n",
        "    except Exception:\n",
        "        best_base, base_idx = base_names[0], 0\n",
        "\n",
        "    row0 = X_test_raw.iloc[[0]]\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre = pipe.named_steps[\"pre\"]; clf = pipe.named_steps[\"clf\"]\n",
        "    X0 = _to_dense(pre.transform(row0)); fn = _feat_names(pre)\n",
        "\n",
        "    if _is_tree_model(clf):\n",
        "        try:\n",
        "            bg0 = shap.kmeans(_to_dense(pre.transform(\n",
        "                X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "            )), 32)\n",
        "        except Exception:\n",
        "            bg0 = _to_dense(pre.transform(\n",
        "                X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "            ))\n",
        "        bg0_arr = _to_ndarray(bg0)\n",
        "\n",
        "        expl0 = shap.TreeExplainer(\n",
        "            clf,\n",
        "            data=bg0_arr,\n",
        "            model_output=\"probability\",\n",
        "            feature_perturbation=\"interventional\"\n",
        "        )\n",
        "        sv0 = expl0.shap_values(X0, check_additivity=False)\n",
        "        if isinstance(sv0, list):\n",
        "            vals0 = np.mean([np.array(v)[0] for v in sv0], axis=0)\n",
        "        else:\n",
        "            vals0 = np.abs(np.array(sv0)[0])\n",
        "    else:\n",
        "        bg0 = _to_dense(pre.transform(\n",
        "            X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE)\n",
        "        ))\n",
        "        bg0_arr = _to_ndarray(bg0)\n",
        "        expl0 = shap.LinearExplainer(clf, bg0_arr)\n",
        "        vals0 = np.abs(np.array(expl0.shap_values(X0))[0])\n",
        "\n",
        "    s = pd.Series(vals0, index=fn).sort_values(ascending=False).head(SINGLE_LOCAL_TOP)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({best_base})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPLAIN_DIR / \"single_customer_top_bar.png\", dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR} (PNGs + CSVs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwlKGB_gu4m3",
        "outputId": "20aaacb3-c218-488e-e459-80dcd6056902"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved: outputs/metrics.json\n",
            "✓ Saved meta SHAP summaries & importances.\n",
            "! SHAP failed for base model xgb: could not convert string to float: '[2.6542976E-1]'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|=================== | 764/800 [00:19<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fast SHAP computed for base model: lgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|=================== | 779/800 [00:17<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fast SHAP computed for base model: cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1599/1600 [03:56<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! SHAP failed for base model rf: Data must be 1-dimensional, got ndarray of shape (45, 2) instead\n",
            "✓ Fast SHAP computed for base model: hgb\n",
            "✓ Saved per-model & aggregated feature importances.\n",
            "✓ Saved single-customer quick chart using base 'cat'.\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain (PNGs + CSVs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + SHAP (Final v2)\n",
        "# Robust to SHAP 0.49.x & sklearn tree shapes\n",
        "# =========================================\n",
        "\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier; HAVE_XGB = True\n",
        "except: HAVE_XGB = False\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier; HAVE_LGB = True\n",
        "except: HAVE_LGB = False\n",
        "try:\n",
        "    from catboost import CatBoostClassifier; HAVE_CAT = True\n",
        "except: HAVE_CAT = False\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "# -------------------------\n",
        "def find_file_recursively(filename, roots=[\"/content\", \".\"]):\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"{filename} not found\")\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME)\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns: df.drop(columns=[\"customerID\"], inplace=True)\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols)\n",
        "], verbose_feature_names_out=False)\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "def get_base_models():\n",
        "    models = {}\n",
        "    if HAVE_XGB:\n",
        "        models[\"xgb\"] = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1)\n",
        "    if HAVE_LGB:\n",
        "        models[\"lgb\"] = LGBMClassifier(n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1)\n",
        "    if HAVE_CAT:\n",
        "        models[\"cat\"] = CatBoostClassifier(iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE)\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF and Test Matrices\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(models, X_tr_raw, y_tr, X_te_raw):\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    n_train, n_test = X_tr_raw.shape[0], X_te_raw.shape[0]\n",
        "    oof, test_meta = np.zeros((n_train,len(models))), np.zeros((n_test,len(models)))\n",
        "    fitted_full_pipes, per_model_scores = [], {}\n",
        "    X_tr_idx, X_te_idx = X_tr_raw.reset_index(drop=True), X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j,(name,clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train)\n",
        "        fold_scores=[]\n",
        "        for tr_idx,va_idx in skf.split(X_tr_idx,y_tr_idx):\n",
        "            X_tr_f,X_va_f=X_tr_idx.iloc[tr_idx],X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f,y_va_f=y_tr_idx[tr_idx],y_tr_idx[va_idx]\n",
        "            pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf)])\n",
        "            pipe.fit(X_tr_f,y_tr_f)\n",
        "            p_va=pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx]=p_va\n",
        "            fold_scores.append((roc_auc_score(y_va_f,p_va),average_precision_score(y_va_f,p_va)))\n",
        "        aucs=[a for a,_ in fold_scores]; prs=[p for _,p in fold_scores]\n",
        "        per_model_scores[name]={\"oof_roc_auc_mean\":float(np.mean(aucs)),\"oof_pr_auc_mean\":float(np.mean(prs))}\n",
        "        oof[:,j]=preds_oof\n",
        "        full_pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf.__class__(**clf.get_params()))])\n",
        "        full_pipe.fit(X_tr_idx,y_tr_idx)\n",
        "        test_meta[:,j]=full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "        print(f\"[{name}] OOF ROC-AUC={np.mean(aucs):.4f} | PR-AUC={np.mean(prs):.4f}\")\n",
        "    return oof,test_meta,per_model_scores,fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix,test_matrix,base_oof_scores,fitted_base_pipes=build_oof_and_test_matrix(base_models,X_train_raw,y_train,X_test_raw)\n",
        "\n",
        "# -------------------------\n",
        "# Meta Learner\n",
        "# -------------------------\n",
        "meta=LogisticRegression(max_iter=2000,class_weight=\"balanced\",random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix,y_train)\n",
        "meta_val_auc=roc_auc_score(y_test,meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr=average_precision_score(y_test,meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "bundle={\"base_pipes\":fitted_base_pipes,\"meta\":meta,\"base_order\":base_names}\n",
        "joblib.dump(bundle,OUT_DIR/\"stacking_bundle.pkl\")\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps({\n",
        "    \"base_oof_scores\":base_oof_scores,\"meta_holdout\":{\"roc_auc\":meta_val_auc,\"pr_auc\":meta_val_pr}},indent=2))\n",
        "print(\"✓ Saved:\",OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# SHAP Explainability\n",
        "# -------------------------\n",
        "def _to_dense(X): return X.toarray() if hasattr(X,\"toarray\") else X\n",
        "def _float32(A): return np.asarray(_to_dense(A),dtype=np.float32)\n",
        "def _is_tree(clf): return any(k in clf.__class__.__name__.lower() for k in [\"xgb\",\"lgbm\",\"cat\",\"forest\",\"gradient\"])\n",
        "def _is_xgb(clf): return clf.__class__.__name__.lower().startswith(\"xgb\")\n",
        "def _names(pre): return pre.get_feature_names_out()\n",
        "\n",
        "def _norm(vals,n_feat):\n",
        "    arr=np.array(vals,dtype=object if isinstance(vals,list) else None)\n",
        "    try: arr=np.array(vals)\n",
        "    except: arr=np.stack([np.asarray(v) for v in vals],axis=0)\n",
        "    arr=np.abs(arr)\n",
        "    if arr.ndim==2: return arr\n",
        "    if arr.ndim==3:\n",
        "        if arr.shape[2]==n_feat: return arr.mean(axis=1)\n",
        "        elif arr.shape[0] in (2,3): return arr.mean(axis=0)\n",
        "    if isinstance(vals,list): return np.stack([np.asarray(v) for v in vals],axis=0).mean(axis=0)\n",
        "    return np.asarray(vals)\n",
        "\n",
        "def _bg(X,k):\n",
        "    try: bg=shap.kmeans(X,k); return _float32(bg.data if hasattr(bg,\"data\") else bg)\n",
        "    except: return _float32(shap.sample(X,min(k*10,X.shape[0])))\n",
        "\n",
        "N_SAMPLE=800; K_BG=64; EXPLAIN_TOP=30\n",
        "X_exp=X_train_raw.sample(n=min(N_SAMPLE,len(X_train_raw)),random_state=RANDOM_STATE)\n",
        "global_imp=defaultdict(list); per_model_top10={}\n",
        "\n",
        "for name,pipe in zip(base_names,fitted_base_pipes):\n",
        "    try:\n",
        "        pre,clf=pipe.named_steps[\"pre\"],pipe.named_steps[\"clf\"]\n",
        "        X_enc=_float32(pre.transform(X_exp)); fn=_names(pre); n_feat=len(fn)\n",
        "        if _is_tree(clf):\n",
        "            bg_arr=_bg(X_enc,K_BG)\n",
        "            expl=shap.TreeExplainer(clf.get_booster() if _is_xgb(clf) else clf,\n",
        "                data=bg_arr,model_output=\"probability\",feature_perturbation=\"interventional\")\n",
        "            vals=_norm(expl.shap_values(X_enc,check_additivity=False),n_feat)\n",
        "        else:\n",
        "            bg_arr=_float32(shap.sample(X_enc,min(128,X_enc.shape[0])))\n",
        "            expl=shap.LinearExplainer(clf,bg_arr)\n",
        "            vals=_norm(expl.shap_values(X_enc),n_feat)\n",
        "        imp=pd.Series(vals.mean(axis=0),index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name]=imp.head(10)\n",
        "        (imp.head(EXPLAIN_TOP).sort_values(ascending=True)\n",
        "          .plot(kind=\"barh\",figsize=(6,8)))\n",
        "        plt.tight_layout(); plt.savefig(EXPLAIN_DIR/f\"base_{name}_bar.png\",dpi=200); plt.close()\n",
        "        for f,v in imp.items(): global_imp[f].append(v)\n",
        "        print(f\"✓ Fast SHAP computed for base model: {name}\")\n",
        "    except Exception as e: print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "if global_imp:\n",
        "    agg={f:np.mean(vs) for f,vs in global_imp.items()}\n",
        "    agg=pd.Series(agg).sort_values(ascending=False)\n",
        "    top10=agg.head(10)\n",
        "    pd.DataFrame(per_model_top10).fillna(0).to_csv(EXPLAIN_DIR/\"per_model_top10.csv\")\n",
        "    agg.to_csv(EXPLAIN_DIR/\"global_feature_importance.csv\",header=[\"mean_abs_shap\"])\n",
        "    (top10.sort_values(ascending=True)\n",
        "     .plot(kind=\"barh\",figsize=(5,4)))\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"global_top10_bar.png\",dpi=200); plt.close()\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else: print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# Single-customer quick chart\n",
        "try:\n",
        "    best_base=base_names[0]\n",
        "    row0=X_test_raw.iloc[[0]]\n",
        "    pipe=fitted_base_pipes[0]\n",
        "    pre,clf=pipe.named_steps[\"pre\"],pipe.named_steps[\"clf\"]\n",
        "    X0=_float32(pre.transform(row0)); fn=_names(pre); n_feat=len(fn)\n",
        "    bg0=_bg(_float32(pre.transform(X_train_raw.sample(400,random_state=RANDOM_STATE))),32)\n",
        "    expl0=shap.TreeExplainer(clf.get_booster() if _is_xgb(clf) else clf,\n",
        "        data=bg0,model_output=\"probability\",feature_perturbation=\"interventional\")\n",
        "    vals0=_norm(expl0.shap_values(X0,check_additivity=False),n_feat)[0]\n",
        "    s=pd.Series(vals0,index=fn).sort_values(ascending=False).head(15)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\",figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions ({best_base})\")\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"single_customer_top_bar.png\",dpi=200); plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{best_base}'.\")\n",
        "except Exception as e: print(f\"! Single-customer quick chart failed: {e}\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QyTZ5X4xx2R",
        "outputId": "418ae74b-e707-4e06-f318-cb9615734c06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "! SHAP failed for base model xgb: could not convert string to float: '[2.6542976E-1]'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|=================== | 766/800 [00:20<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fast SHAP computed for base model: lgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|=================== | 751/800 [00:17<00:01]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fast SHAP computed for base model: cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1597/1600 [04:31<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! SHAP failed for base model rf: Data must be 1-dimensional, got ndarray of shape (45, 2) instead\n",
            "✓ Fast SHAP computed for base model: hgb\n",
            "✓ Saved per-model & aggregated feature importances.\n",
            "! Single-customer quick chart failed: could not convert string to float: '[2.6542976E-1]'\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# SHAP (final robust block): handles XGB fallback + RF shapes\n",
        "# Replace your SHAP section with this one\n",
        "# =======================================================\n",
        "import shap, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def _to_dense(X): return X.toarray() if hasattr(X,\"toarray\") else X\n",
        "def _float32(A): return np.asarray(_to_dense(A), dtype=np.float32)\n",
        "def _is_tree(clf): return any(k in clf.__class__.__name__.lower() for k in [\"xgb\",\"lgbm\",\"cat\",\"forest\",\"gradient\"])\n",
        "def _is_xgb(clf): return clf.__class__.__name__.lower().startswith(\"xgb\")\n",
        "def _names(pre): return pre.get_feature_names_out()\n",
        "\n",
        "def _bg(X, k, fallback_mult=10):\n",
        "    # Try kmeans, else random sample; always ndarray float32\n",
        "    try:\n",
        "        bg = shap.kmeans(X, k)\n",
        "        return _float32(bg.data if hasattr(bg,\"data\") else bg)\n",
        "    except Exception:\n",
        "        return _float32(shap.sample(X, min(k*fallback_mult, X.shape[0])))\n",
        "\n",
        "def _norm(vals, n_feat):\n",
        "    \"\"\"Return (n_samples, n_features) no matter what the explainer returned.\"\"\"\n",
        "    # Try plain array first\n",
        "    try:\n",
        "        arr = np.array(vals)\n",
        "    except Exception:\n",
        "        arr = np.stack([np.asarray(v) for v in vals], axis=0)  # list-of-classes\n",
        "    arr = np.abs(arr)\n",
        "\n",
        "    if arr.ndim == 2:\n",
        "        # (n_samples, n_features)  ✅\n",
        "        return arr\n",
        "\n",
        "    if arr.ndim == 3:\n",
        "        # Handle three common layouts:\n",
        "        # (n_samples, n_classes, n_features) → mean over classes\n",
        "        if arr.shape[2] == n_feat and arr.shape[1] in (2,3):\n",
        "            return arr.mean(axis=1)\n",
        "        # (n_classes, n_samples, n_features) → mean over classes\n",
        "        if arr.shape[0] in (2,3) and arr.shape[2] == n_feat:\n",
        "            return arr.mean(axis=0)\n",
        "        # (n_samples, n_features, n_classes) → mean over classes (last axis)\n",
        "        if arr.shape[1] == n_feat and arr.shape[2] in (2,3):\n",
        "            return arr.mean(axis=2)\n",
        "\n",
        "    # list of per-class arrays (n_classes, n_samples, n_features)\n",
        "    if isinstance(vals, list):\n",
        "        stacked = np.stack([np.asarray(v) for v in vals], axis=0)\n",
        "        return np.abs(stacked).mean(axis=0)\n",
        "\n",
        "    # Fallback: assume already (n_samples, n_features)\n",
        "    return np.asarray(vals)\n",
        "\n",
        "# ---- (1) Meta-level SHAP (base learners as features) ----\n",
        "explainer_meta = shap.LinearExplainer(meta, oof_matrix)\n",
        "sv_meta = explainer_meta.shap_values(test_matrix)  # (n_test, n_models)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(sv_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR / \"meta_summary_base_contributions.png\", dpi=200); plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(sv_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR / \"meta_summary_bar_base_contributions.png\", dpi=200); plt.close()\n",
        "\n",
        "pd.Series(np.mean(np.abs(sv_meta), axis=0), index=base_names)\\\n",
        "  .sort_values(ascending=False)\\\n",
        "  .to_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- (2) Per-base model SHAP on original features + aggregation ----\n",
        "N_SAMPLE = min(800, len(X_train_raw))\n",
        "K_BG = 64\n",
        "EXPLAIN_TOP_BAR = 30\n",
        "\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "successful_base = None  # to pick a model for single-customer chart\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre, clf = pipe.named_steps[\"pre\"], pipe.named_steps[\"clf\"]\n",
        "        X_enc = _float32(pre.transform(X_for_explain))\n",
        "        fn = _names(pre); n_feat = len(fn)\n",
        "\n",
        "        if _is_tree(clf):\n",
        "            bg_arr = _bg(X_enc, K_BG)\n",
        "\n",
        "            # Prefer TreeExplainer; for XGB fallback to Kernel if it throws conversion errors\n",
        "            try:\n",
        "                model_for_shap = clf.get_booster() if _is_xgb(clf) else clf\n",
        "                expl = shap.TreeExplainer(\n",
        "                    model_for_shap,\n",
        "                    data=bg_arr,\n",
        "                    model_output=\"probability\",\n",
        "                    feature_perturbation=\"interventional\"\n",
        "                )\n",
        "                vals_raw = expl.shap_values(X_enc, check_additivity=False)\n",
        "                vals = _norm(vals_raw, n_feat)\n",
        "            except Exception as e_xgb:\n",
        "                if _is_xgb(clf):\n",
        "                    # Robust fallback: KernelExplainer on a smaller batch (fast enough)\n",
        "                    print(f\"… XGB TreeExplainer failed; falling back to KernelExplainer: {e_xgb}\")\n",
        "                    bg_arr = _bg(X_enc, 32)\n",
        "                    pred = (lambda data: clf.predict_proba(data)[:,1])\n",
        "                    expl = shap.KernelExplainer(pred, bg_arr)\n",
        "                    X_batch = _float32(shap.sample(X_enc, min(300, X_enc.shape[0])))\n",
        "                    vals_raw = expl.shap_values(X_batch)\n",
        "                    vals = _norm(vals_raw, n_feat)\n",
        "                    # Align to full sample length by repeating mean (keeps outputs consistent for saving charts)\n",
        "                    if vals.shape[0] != X_enc.shape[0]:\n",
        "                        vals = np.repeat(vals.mean(axis=0, keepdims=True), X_enc.shape[0], axis=0)\n",
        "                else:\n",
        "                    raise\n",
        "        else:\n",
        "            # LinearExplainer with small bg; fallback to Kernel if needed\n",
        "            try:\n",
        "                bg_arr = _float32(shap.sample(X_enc, min(128, X_enc.shape[0])))\n",
        "                expl = shap.LinearExplainer(clf, bg_arr)\n",
        "                vals_raw = expl.shap_values(X_enc)\n",
        "                vals = _norm(vals_raw, n_feat)\n",
        "            except Exception:\n",
        "                bg_arr = _bg(X_enc, 32)\n",
        "                pred = (lambda data: clf.predict_proba(data)[:,1])\n",
        "                expl = shap.KernelExplainer(pred, bg_arr)\n",
        "                X_batch = _float32(shap.sample(X_enc, min(400, X_enc.shape[0])))\n",
        "                vals_raw = expl.shap_values(X_batch)\n",
        "                vals = _norm(vals_raw, n_feat)\n",
        "                if vals.shape[0] != X_enc.shape[0]:\n",
        "                    vals = np.repeat(vals.mean(axis=0, keepdims=True), X_enc.shape[0], axis=0)\n",
        "\n",
        "        imp = pd.Series(vals.mean(axis=0), index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        (imp.head(EXPLAIN_TOP_BAR).sort_values(ascending=True)\n",
        "           .plot(kind=\"barh\", figsize=(6, 8)))\n",
        "        plt.tight_layout(); plt.savefig(EXPLAIN_DIR / f\"base_{name}_bar.png\", dpi=200); plt.close()\n",
        "\n",
        "        for f, v in imp.items(): global_feat_importance[f].append(v)\n",
        "\n",
        "        if successful_base is None:\n",
        "            successful_base = name  # remember first model that succeeded for local chart\n",
        "\n",
        "        print(f\"✓ SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models (mean |SHAP|)\n",
        "if global_feat_importance:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10_all = agg.head(10)\n",
        "\n",
        "    pd.DataFrame(per_model_top10).fillna(0.0).to_csv(EXPLAIN_DIR / \"per_model_top10.csv\")\n",
        "    agg.to_csv(EXPLAIN_DIR / \"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    top10_all.to_csv(EXPLAIN_DIR / \"global_top10_features.csv\", header=[\"mean_abs_shap\"])\n",
        "\n",
        "    (top10_all.sort_values(ascending=True)\n",
        "        .plot(kind=\"barh\", figsize=(5, 4)))\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR / \"global_top10_bar.png\", dpi=200); plt.close()\n",
        "\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "else:\n",
        "    print(\"! No per-model SHAP importances aggregated.\")\n",
        "\n",
        "# ---- (3) Single-customer quick local chart (choose a successful model; avoid XGB path if it failed) ----\n",
        "try:\n",
        "    # Pick a model that succeeded; fallback to strongest by meta if available\n",
        "    if successful_base is None:\n",
        "        try:\n",
        "            meta_imp = pd.read_csv(EXPLAIN_DIR / \"meta_base_importance.csv\", index_col=0).iloc[:,0]\n",
        "            successful_base = meta_imp.idxmax()\n",
        "        except Exception:\n",
        "            successful_base = base_names[0]\n",
        "\n",
        "    base_idx = base_names.index(successful_base)\n",
        "    pipe = fitted_base_pipes[base_idx]\n",
        "    pre, clf = pipe.named_steps[\"pre\"], pipe.named_steps[\"clf\"]\n",
        "    fn = _names(pre); n_feat = len(fn)\n",
        "\n",
        "    X0 = _float32(pre.transform(X_test_raw.iloc[[0]]))\n",
        "    bg0 = _bg(_float32(pre.transform(X_train_raw.sample(min(400, len(X_train_raw)), random_state=RANDOM_STATE))), 32)\n",
        "\n",
        "    vals0 = None\n",
        "    if _is_tree(clf):\n",
        "        try:\n",
        "            model_for_shap = clf.get_booster() if _is_xgb(clf) else clf\n",
        "            expl0 = shap.TreeExplainer(model_for_shap, data=bg0, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "            vraw = expl0.shap_values(X0, check_additivity=False)\n",
        "            vals0 = _norm(vraw, n_feat)[0]\n",
        "        except Exception as e_xgb:\n",
        "            # Kernel fallback for local chart (fast)\n",
        "            pred = (lambda data: clf.predict_proba(data)[:,1])\n",
        "            expl0 = shap.KernelExplainer(pred, bg0)\n",
        "            vraw = expl0.shap_values(X0)\n",
        "            vals0 = _norm(vraw, n_feat)[0]\n",
        "    else:\n",
        "        try:\n",
        "            expl0 = shap.LinearExplainer(clf, bg0)\n",
        "            vraw = expl0.shap_values(X0)\n",
        "            vals0 = _norm(vraw, n_feat)[0]\n",
        "        except Exception:\n",
        "            pred = (lambda data: clf.predict_proba(data)[:,1])\n",
        "            expl0 = shap.KernelExplainer(pred, bg0)\n",
        "            vraw = expl0.shap_values(X0)\n",
        "            vals0 = _norm(vraw, n_feat)[0]\n",
        "\n",
        "    s = pd.Series(vals0, index=fn).sort_values(ascending=False).head(15)\n",
        "    s.sort_values(ascending=True).plot(kind=\"barh\", figsize=(6,5))\n",
        "    plt.title(f\"Top local contributions for one customer ({successful_base})\")\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR / \"single_customer_top_bar.png\", dpi=200); plt.close()\n",
        "    print(f\"✓ Saved single-customer quick chart using base '{successful_base}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"! Single-customer quick chart failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "1043774cd3ca4a09919eb1b15cbb80ad",
            "8c3164343fe24bf6a41e15fd3a34a616",
            "78ed8268690a4b0d802fdb12ae7f7b23",
            "4b2e1f3a840c440f956ad54eb3a509d1",
            "60ce60ba22be4b559fb548418b42b0f5",
            "562972d0449d44cfa88e6bdc7c87b595",
            "7f2220a53d9342d1b6519a33c467f3d3",
            "d8bf507e5d344ab6a9639dbefb3e3337",
            "e0c400a3051a4937b1e040507c0bd42e",
            "fd3e963388a34c8f9472bbcc68137368",
            "b87e1143f6bf43518755b9d58eb91281",
            "3ea9eb59cf404ac198e658e9532ba3df",
            "11b586670f5746cba724a1cb5f825920",
            "0e2b49c4be12413f8e82e94faa551a8f",
            "ef9a16b8330642079dc5b085a8fdd7ef",
            "7a56dc752da34de6a9535b6a3c3e1d9c",
            "89824b6eb44a408ab93424a17ac5a63f",
            "5e0da990cde74269ad1fd9e304802522",
            "f7613a482425424b98270825d78fa26f",
            "d0528d5b5d604b7e9b68424802bf18e9",
            "be0e0e9b51ae4ed0b6f69283b2dd35fb",
            "8d9b6f3b36e440e8a9a18b0a9e6cec20"
          ]
        },
        "id": "Y0rhaoY00aVO",
        "outputId": "2ba27a75-4eb2-4b8a-83f5-df13d725df54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved meta SHAP summaries & importances.\n",
            "… XGB TreeExplainer failed; falling back to KernelExplainer: could not convert string to float: '[2.6542976E-1]'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1043774cd3ca4a09919eb1b15cbb80ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: xgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|===================| 782/800 [00:19<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: lgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|===================| 791/800 [00:18<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1595/1600 [04:21<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: rf\n",
            "✓ SHAP computed for base model: hgb\n",
            "✓ Saved per-model & aggregated feature importances.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ea9eb59cf404ac198e658e9532ba3df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved single-customer quick chart using base 'xgb'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + SHAP (Force XGB Kernel)\n",
        "# =========================================\n",
        "\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# optional imports\n",
        "try:\n",
        "    from xgboost import XGBClassifier; HAVE_XGB = True\n",
        "except: HAVE_XGB = False\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier; HAVE_LGB = True\n",
        "except: HAVE_LGB = False\n",
        "try:\n",
        "    from catboost import CatBoostClassifier; HAVE_CAT = True\n",
        "except: HAVE_CAT = False\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "# -------------------------\n",
        "def find_file_recursively(filename, roots=[\"/content\", \".\"]):\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"{filename} not found\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME)\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns: df.drop(columns=[\"customerID\"], inplace=True)\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols)\n",
        "], verbose_feature_names_out=False)\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "def get_base_models():\n",
        "    models = {}\n",
        "    if HAVE_XGB:\n",
        "        models[\"xgb\"] = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1)\n",
        "    if HAVE_LGB:\n",
        "        models[\"lgb\"] = LGBMClassifier(n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1)\n",
        "    if HAVE_CAT:\n",
        "        models[\"cat\"] = CatBoostClassifier(iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE)\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF and Test Matrices\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(models, X_tr_raw, y_tr, X_te_raw):\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    n_train, n_test = X_tr_raw.shape[0], X_te_raw.shape[0]\n",
        "    oof, test_meta = np.zeros((n_train,len(models))), np.zeros((n_test,len(models)))\n",
        "    fitted_full_pipes, per_model_scores = [], {}\n",
        "    X_tr_idx, X_te_idx = X_tr_raw.reset_index(drop=True), X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j,(name,clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train)\n",
        "        fold_scores=[]\n",
        "        for tr_idx,va_idx in skf.split(X_tr_idx,y_tr_idx):\n",
        "            X_tr_f,X_va_f=X_tr_idx.iloc[tr_idx],X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f,y_va_f=y_tr_idx[tr_idx],y_tr_idx[va_idx]\n",
        "            pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf)])\n",
        "            pipe.fit(X_tr_f,y_tr_f)\n",
        "            p_va=pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx]=p_va\n",
        "            fold_scores.append((roc_auc_score(y_va_f,p_va),average_precision_score(y_va_f,p_va)))\n",
        "        aucs=[a for a,_ in fold_scores]; prs=[p for _,p in fold_scores]\n",
        "        per_model_scores[name]={\"oof_roc_auc_mean\":float(np.mean(aucs)),\"oof_pr_auc_mean\":float(np.mean(prs))}\n",
        "        oof[:,j]=preds_oof\n",
        "        full_pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf.__class__(**clf.get_params()))])\n",
        "        full_pipe.fit(X_tr_idx,y_tr_idx)\n",
        "        test_meta[:,j]=full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "        print(f\"[{name}] OOF ROC-AUC={np.mean(aucs):.4f} | PR-AUC={np.mean(prs):.4f}\")\n",
        "    return oof,test_meta,per_model_scores,fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix,test_matrix,base_oof_scores,fitted_base_pipes=build_oof_and_test_matrix(base_models,X_train_raw,y_train,X_test_raw)\n",
        "\n",
        "# -------------------------\n",
        "# Meta Learner\n",
        "# -------------------------\n",
        "meta=LogisticRegression(max_iter=2000,class_weight=\"balanced\",random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix,y_train)\n",
        "meta_val_auc=roc_auc_score(y_test,meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr=average_precision_score(y_test,meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "bundle={\"base_pipes\":fitted_base_pipes,\"meta\":meta,\"base_order\":base_names}\n",
        "joblib.dump(bundle,OUT_DIR/\"stacking_bundle.pkl\")\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps({\n",
        "    \"base_oof_scores\":base_oof_scores,\"meta_holdout\":{\"roc_auc\":meta_val_auc,\"pr_auc\":meta_val_pr}},indent=2))\n",
        "print(\"✓ Saved:\",OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# SHAP Explainability (Force XGB Kernel)\n",
        "# -------------------------\n",
        "FORCE_XGB_KERNEL = True\n",
        "def _to_dense(X): return X.toarray() if hasattr(X,\"toarray\") else X\n",
        "def _float32(A): return np.asarray(_to_dense(A), dtype=np.float32)\n",
        "def _is_tree(clf): return any(k in clf.__class__.__name__.lower() for k in [\"xgb\",\"lgbm\",\"cat\",\"forest\",\"gradient\"])\n",
        "def _is_xgb(clf): return clf.__class__.__name__.lower().startswith(\"xgb\")\n",
        "def _names(pre): return pre.get_feature_names_out()\n",
        "def _bg(X,k):\n",
        "    try: bg=shap.kmeans(X,k); return _float32(bg.data if hasattr(bg,\"data\") else bg)\n",
        "    except: return _float32(shap.sample(X,min(k*10,X.shape[0])))\n",
        "def _norm(vals,n_feat):\n",
        "    arr=np.array(vals,dtype=object if isinstance(vals,list) else None)\n",
        "    try: arr=np.array(vals)\n",
        "    except: arr=np.stack([np.asarray(v) for v in vals],axis=0)\n",
        "    arr=np.abs(arr)\n",
        "    if arr.ndim==2: return arr\n",
        "    if arr.ndim==3:\n",
        "        if arr.shape[2]==n_feat: return arr.mean(axis=1)\n",
        "        elif arr.shape[0] in (2,3): return arr.mean(axis=0)\n",
        "    if isinstance(vals,list): return np.stack([np.asarray(v) for v in vals],axis=0).mean(axis=0)\n",
        "    return np.asarray(vals)\n",
        "\n",
        "# meta-level\n",
        "explainer_meta=shap.LinearExplainer(meta,oof_matrix)\n",
        "sv_meta=explainer_meta.shap_values(test_matrix)\n",
        "plt.figure(); shap.summary_plot(sv_meta,feature_names=base_names,show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_base_contributions.png\",dpi=200); plt.close()\n",
        "plt.figure(); shap.summary_plot(sv_meta,feature_names=base_names,plot_type=\"bar\",show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_bar_base_contributions.png\",dpi=200); plt.close()\n",
        "pd.Series(np.mean(np.abs(sv_meta),axis=0),index=base_names).sort_values(ascending=False)\\\n",
        "    .to_csv(EXPLAIN_DIR/\"meta_base_importance.csv\",header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# per-base\n",
        "N_SAMPLE=min(800,len(X_train_raw)); X_exp=X_train_raw.sample(n=N_SAMPLE,random_state=RANDOM_STATE)\n",
        "global_imp=defaultdict(list); per_model_top10={}\n",
        "for name,pipe in zip(base_names,fitted_base_pipes):\n",
        "    try:\n",
        "        pre,clf=pipe.named_steps[\"pre\"],pipe.named_steps[\"clf\"]\n",
        "        X_enc=_float32(pre.transform(X_exp)); fn=_names(pre); n_feat=len(fn)\n",
        "        if _is_tree(clf):\n",
        "            if _is_xgb(clf) and FORCE_XGB_KERNEL:\n",
        "                bg_arr=_bg(X_enc,32); pred=lambda data: clf.predict_proba(data)[:,1]\n",
        "                expl=shap.KernelExplainer(pred,bg_arr)\n",
        "                X_batch=_float32(shap.sample(X_enc,min(300,X_enc.shape[0])))\n",
        "                vals_raw=expl.shap_values(X_batch); vals=_norm(vals_raw,n_feat)\n",
        "                if vals.shape[0]!=X_enc.shape[0]: vals=np.repeat(vals.mean(axis=0,keepdims=True),X_enc.shape[0],axis=0)\n",
        "            else:\n",
        "                bg_arr=_bg(X_enc,64)\n",
        "                expl=shap.TreeExplainer(clf,data=bg_arr,model_output=\"probability\",feature_perturbation=\"interventional\")\n",
        "                vals=_norm(expl.shap_values(X_enc,check_additivity=False),n_feat)\n",
        "        else:\n",
        "            bg_arr=_bg(X_enc,64)\n",
        "            expl=shap.LinearExplainer(clf,bg_arr)\n",
        "            vals=_norm(expl.shap_values(X_enc),n_feat)\n",
        "        imp=pd.Series(vals.mean(axis=0),index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name]=imp.head(10)\n",
        "        imp.head(30).sort_values(ascending=True).plot(kind=\"barh\",figsize=(6,8))\n",
        "        plt.tight_layout(); plt.savefig(EXPLAIN_DIR/f\"base_{name}_bar.png\",dpi=200); plt.close()\n",
        "        for f,v in imp.items(): global_imp[f].append(v)\n",
        "        print(f\"✓ SHAP computed for base model: {name}\")\n",
        "    except Exception as e: print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "if global_imp:\n",
        "    agg={f:np.mean(vs) for f,vs in global_imp.items()}\n",
        "    agg=pd.Series(agg).sort_values(ascending=False)\n",
        "    agg.to_csv(EXPLAIN_DIR/\"global_feature_importance.csv\",header=[\"mean_abs_shap\"])\n",
        "    agg.head(10).sort_values(ascending=True).plot(kind=\"barh\",figsize=(5,4))\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"global_top10_bar.png\",dpi=200); plt.close()\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "06112468c06340fca78119e62cda0b5c",
            "e4623c1e33ef402997e8c0cefa774403",
            "12b95b8c3f2d4af88782442b468ad807",
            "33da78ce82914397a04db1dcd72652bf",
            "50725bf232a14380b117ba18cfc488f6",
            "3fbe3d5d18d7436983e3df24aad5f383",
            "927a70937d464868adf3af4d8818677c",
            "0ff41f0357ce45618e9eb7eeaf99a85f",
            "c7c397013f09495baefee1741add0405",
            "10b05227b2a441d582a01ddb86dc90ca",
            "e3f0758221344585aa2b5ffe6b1ca87f"
          ]
        },
        "id": "_FFe4xbk3hAw",
        "outputId": "bd7a9f98-ca72-45a7-b7ef-a614ef2f04b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved meta SHAP summaries & importances.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06112468c06340fca78119e62cda0b5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: xgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|===================| 780/800 [00:19<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: lgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|=================== | 770/800 [00:17<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1593/1600 [04:19<00:01]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! SHAP failed for base model rf: Data must be 1-dimensional, got ndarray of shape (45, 2) instead\n",
            "✓ SHAP computed for base model: hgb\n",
            "✓ Saved per-model & aggregated feature importances.\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# ONE-CELL COLAB: STACKING + SHAP (Force XGB Kernel + RF Safety Squeeze)\n",
        "# =========================================\n",
        "\n",
        "!pip -q install numpy pandas scikit-learn xgboost lightgbm catboost shap joblib matplotlib\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# Optional boosters\n",
        "try:\n",
        "    from xgboost import XGBClassifier; HAVE_XGB = True\n",
        "except: HAVE_XGB = False\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier; HAVE_LGB = True\n",
        "except: HAVE_LGB = False\n",
        "try:\n",
        "    from catboost import CatBoostClassifier; HAVE_CAT = True\n",
        "except: HAVE_CAT = False\n",
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_FILE_NAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "TARGET = \"Churn\"\n",
        "TEST_SIZE = 0.25\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "FORCE_XGB_KERNEL = True  # نستخدم KernelExplainer دائمًا مع XGB\n",
        "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "EXPLAIN_DIR = OUT_DIR / \"explain\"; EXPLAIN_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "# -------------------------\n",
        "def find_file_recursively(filename, roots=[\"/content\", \".\"]):\n",
        "    for root in roots:\n",
        "        for r, _, files in os.walk(root):\n",
        "            if filename in files:\n",
        "                return os.path.join(r, filename)\n",
        "    raise FileNotFoundError(f\"{filename} not found\")\n",
        "\n",
        "csv_path = find_file_recursively(CSV_FILE_NAME)\n",
        "print(f\"✓ Found dataset at: {csv_path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"TotalCharges\" in df.columns:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").fillna(0.0)\n",
        "if \"customerID\" in df.columns: df.drop(columns=[\"customerID\"], inplace=True)\n",
        "y = df[TARGET].astype(str).str.strip().map({\"Yes\":1,\"No\":0}).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols)\n",
        "], verbose_feature_names_out=False)\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "def get_base_models():\n",
        "    models = {}\n",
        "    if HAVE_XGB:\n",
        "        models[\"xgb\"] = XGBClassifier(\n",
        "            n_estimators=300, max_depth=6, learning_rate=0.06,\n",
        "            subsample=0.85, colsample_bytree=0.85, eval_metric=\"logloss\",\n",
        "            random_state=RANDOM_STATE, tree_method=\"hist\", n_jobs=-1\n",
        "        )\n",
        "    if HAVE_LGB:\n",
        "        models[\"lgb\"] = LGBMClassifier(\n",
        "            n_estimators=350, num_leaves=64, learning_rate=0.05,\n",
        "            subsample=0.85, colsample_bytree=0.85, random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1\n",
        "        )\n",
        "    if HAVE_CAT:\n",
        "        models[\"cat\"] = CatBoostClassifier(\n",
        "            iterations=400, depth=6, learning_rate=0.05,\n",
        "            loss_function=\"Logloss\", verbose=False, random_state=RANDOM_STATE\n",
        "        )\n",
        "    models[\"rf\"] = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "    models[\"hgb\"] = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.08, random_state=RANDOM_STATE)\n",
        "    return models\n",
        "\n",
        "base_models = get_base_models()\n",
        "base_names = list(base_models.keys())\n",
        "print(\"Base learners:\", base_names)\n",
        "\n",
        "# -------------------------\n",
        "# Build OOF & Test Matrices\n",
        "# -------------------------\n",
        "def build_oof_and_test_matrix(models, X_tr_raw, y_tr, X_te_raw):\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    n_train, n_test = X_tr_raw.shape[0], X_te_raw.shape[0]\n",
        "    oof, test_meta = np.zeros((n_train,len(models))), np.zeros((n_test,len(models)))\n",
        "    fitted_full_pipes, per_model_scores = [], {}\n",
        "    X_tr_idx, X_te_idx = X_tr_raw.reset_index(drop=True), X_te_raw.reset_index(drop=True)\n",
        "    y_tr_idx = pd.Series(y_tr).reset_index(drop=True).values\n",
        "\n",
        "    for j,(name,clf) in enumerate(models.items()):\n",
        "        preds_oof = np.zeros(n_train)\n",
        "        fold_scores=[]\n",
        "        for tr_idx,va_idx in skf.split(X_tr_idx,y_tr_idx):\n",
        "            X_tr_f,X_va_f=X_tr_idx.iloc[tr_idx],X_tr_idx.iloc[va_idx]\n",
        "            y_tr_f,y_va_f=y_tr_idx[tr_idx],y_tr_idx[va_idx]\n",
        "            pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf)])\n",
        "            pipe.fit(X_tr_f,y_tr_f)\n",
        "            p_va=pipe.predict_proba(X_va_f)[:,1]\n",
        "            preds_oof[va_idx]=p_va\n",
        "            fold_scores.append((roc_auc_score(y_va_f,p_va),average_precision_score(y_va_f,p_va)))\n",
        "        aucs=[a for a,_ in fold_scores]; prs=[p for _,p in fold_scores]\n",
        "        per_model_scores[name]={\"oof_roc_auc_mean\":float(np.mean(aucs)),\"oof_pr_auc_mean\":float(np.mean(prs))}\n",
        "        oof[:,j]=preds_oof\n",
        "        full_pipe=Pipeline([(\"pre\",preprocessor),(\"clf\",clf.__class__(**clf.get_params()))])\n",
        "        full_pipe.fit(X_tr_idx,y_tr_idx)\n",
        "        test_meta[:,j]=full_pipe.predict_proba(X_te_idx)[:,1]\n",
        "        fitted_full_pipes.append(full_pipe)\n",
        "        print(f\"[{name}] OOF ROC-AUC={np.mean(aucs):.4f} | PR-AUC={np.mean(prs):.4f}\")\n",
        "    return oof,test_meta,per_model_scores,fitted_full_pipes\n",
        "\n",
        "print(\"→ Rebuilding OOF & test matrices ...\")\n",
        "oof_matrix,test_matrix,base_oof_scores,fitted_base_pipes = build_oof_and_test_matrix(base_models,X_train_raw,y_train,X_test_raw)\n",
        "\n",
        "# -------------------------\n",
        "# Meta Learner\n",
        "# -------------------------\n",
        "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
        "meta.fit(oof_matrix,y_train)\n",
        "meta_val_auc = roc_auc_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "meta_val_pr  = average_precision_score(y_test, meta.predict_proba(test_matrix)[:,1])\n",
        "print(f\"Meta holdout ROC-AUC={meta_val_auc:.4f} | PR-AUC={meta_val_pr:.4f}\")\n",
        "\n",
        "joblib.dump({\"base_pipes\":fitted_base_pipes,\"meta\":meta,\"base_order\":base_names}, OUT_DIR/\"stacking_bundle.pkl\")\n",
        "(OUT_DIR/\"metrics.json\").write_text(json.dumps({\n",
        "    \"base_oof_scores\":base_oof_scores,\n",
        "    \"meta_holdout\":{\"roc_auc\":float(meta_val_auc),\"pr_auc\":float(meta_val_pr)}\n",
        "}, indent=2))\n",
        "print(\"✓ Saved:\", OUT_DIR/\"stacking_bundle.pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# SHAP Explainability (Force XGB Kernel + RF Safety Squeeze)\n",
        "# -------------------------\n",
        "def _to_dense(X): return X.toarray() if hasattr(X,\"toarray\") else X\n",
        "def _float32(A): return np.asarray(_to_dense(A), dtype=np.float32)\n",
        "def _is_tree(clf): return any(k in clf.__class__.__name__.lower() for k in [\"xgb\",\"lgbm\",\"cat\",\"forest\",\"gradient\"])\n",
        "def _is_xgb(clf): return clf.__class__.__name__.lower().startswith(\"xgb\")\n",
        "def _names(pre): return pre.get_feature_names_out()\n",
        "def _bg(X,k):\n",
        "    try:\n",
        "        bg = shap.kmeans(X,k)\n",
        "        return _float32(bg.data if hasattr(bg,\"data\") else bg)\n",
        "    except:\n",
        "        return _float32(shap.sample(X, min(k*10, X.shape[0])))\n",
        "\n",
        "def _norm(vals, n_feat):\n",
        "    # يُوحِّد الشكل إلى (n_samples, n_features)\n",
        "    try: arr = np.array(vals)\n",
        "    except: arr = np.stack([np.asarray(v) for v in vals], axis=0)\n",
        "    arr = np.abs(arr)\n",
        "    if arr.ndim == 2: return arr\n",
        "    if arr.ndim == 3:\n",
        "        # (N, C, F)\n",
        "        if arr.shape[2] == n_feat and arr.shape[1] in (2,3): return arr.mean(axis=1)\n",
        "        # (C, N, F)\n",
        "        if arr.shape[0] in (2,3) and arr.shape[2] == n_feat: return arr.mean(axis=0)\n",
        "        # (N, F, C)\n",
        "        if arr.shape[1] == n_feat and arr.shape[2] in (2,3): return arr.mean(axis=2)\n",
        "    if isinstance(vals, list):\n",
        "        stacked = np.stack([np.asarray(v) for v in vals], axis=0)\n",
        "        return np.abs(stacked).mean(axis=0)\n",
        "    return np.asarray(vals)\n",
        "\n",
        "# ---- (1) Meta-level SHAP (base learners as features) ----\n",
        "explainer_meta = shap.LinearExplainer(meta, oof_matrix)\n",
        "sv_meta = explainer_meta.shap_values(test_matrix)\n",
        "\n",
        "plt.figure(); shap.summary_plot(sv_meta, feature_names=base_names, show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_base_contributions.png\", dpi=200); plt.close()\n",
        "plt.figure(); shap.summary_plot(sv_meta, feature_names=base_names, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"meta_summary_bar_base_contributions.png\", dpi=200); plt.close()\n",
        "pd.Series(np.mean(np.abs(sv_meta), axis=0), index=base_names).sort_values(ascending=False)\\\n",
        "  .to_csv(EXPLAIN_DIR/\"meta_base_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "print(\"✓ Saved meta SHAP summaries & importances.\")\n",
        "\n",
        "# ---- (2) Per-base model SHAP + RF safety squeeze ----\n",
        "N_SAMPLE = min(800, len(X_train_raw))\n",
        "K_BG = 64\n",
        "EXPLAIN_TOP_BAR = 30\n",
        "\n",
        "X_for_explain = X_train_raw.sample(n=N_SAMPLE, random_state=RANDOM_STATE)\n",
        "global_feat_importance = defaultdict(list)\n",
        "per_model_top10 = {}\n",
        "\n",
        "for name, pipe in zip(base_names, fitted_base_pipes):\n",
        "    try:\n",
        "        pre, clf = pipe.named_steps[\"pre\"], pipe.named_steps[\"clf\"]\n",
        "        X_enc = _float32(pre.transform(X_for_explain))\n",
        "        fn = _names(pre); n_feat = len(fn)\n",
        "\n",
        "        if _is_tree(clf):\n",
        "            if _is_xgb(clf) and FORCE_XGB_KERNEL:\n",
        "                # XGB: KernelExplainer ثابت\n",
        "                bg_arr = _bg(X_enc, 32)\n",
        "                pred = lambda data: clf.predict_proba(data)[:,1]\n",
        "                expl = shap.KernelExplainer(pred, bg_arr)\n",
        "                X_batch = _float32(shap.sample(X_enc, min(300, X_enc.shape[0])))\n",
        "                vals_raw = expl.shap_values(X_batch)\n",
        "                vals = _norm(vals_raw, n_feat)\n",
        "                if vals.shape[0] != X_enc.shape[0]:\n",
        "                    vals = np.repeat(vals.mean(axis=0, keepdims=True), X_enc.shape[0], axis=0)\n",
        "            else:\n",
        "                # باقي الشجر: TreeExplainer سريع\n",
        "                bg_arr = _bg(X_enc, K_BG)\n",
        "                expl = shap.TreeExplainer(clf, data=bg_arr, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
        "                vals = _norm(expl.shap_values(X_enc, check_additivity=False), n_feat)\n",
        "        else:\n",
        "            # LinearExplainer (سريع)؛ fallback Kernel لو لزم\n",
        "            try:\n",
        "                bg_arr = _bg(X_enc, 64)\n",
        "                expl = shap.LinearExplainer(clf, bg_arr)\n",
        "                vals = _norm(expl.shap_values(X_enc), n_feat)\n",
        "            except Exception:\n",
        "                bg_arr = _bg(X_enc, 32)\n",
        "                pred = lambda data: clf.predict_proba(data)[:,1]\n",
        "                expl = shap.KernelExplainer(pred, bg_arr)\n",
        "                X_batch = _float32(shap.sample(X_enc, min(400, X_enc.shape[0])))\n",
        "                vals_raw = expl.shap_values(X_batch)\n",
        "                vals = _norm(vals_raw, n_feat)\n",
        "                if vals.shape[0] != X_enc.shape[0]:\n",
        "                    vals = np.repeat(vals.mean(axis=0, keepdims=True), X_enc.shape[0], axis=0)\n",
        "\n",
        "        # ===== RF SAFETY SQUEEZE (يعالج أشكال الأبعاد الغريبة) =====\n",
        "        if \"forest\" in clf.__class__.__name__.lower():\n",
        "            # 3D -> متوسط عبر محور الكلاسات أيًا كان مكانه\n",
        "            if vals.ndim == 3 and vals.shape[2] in (2,3): vals = vals.mean(axis=2)\n",
        "            if vals.ndim == 3 and vals.shape[0] in (2,3) and vals.shape[2] == n_feat: vals = vals.mean(axis=0)\n",
        "            if vals.ndim == 3 and vals.shape[1] == n_feat and vals.shape[2] in (2,3): vals = vals.mean(axis=2)\n",
        "            # (n_features, n_classes) -> (1, n_features)\n",
        "            if vals.ndim == 2 and vals.shape[0] == n_feat and vals.shape[1] in (2,3):\n",
        "                vals = vals.mean(axis=1, keepdims=True)\n",
        "            # (n_features, n_samples) -> transpose\n",
        "            if vals.ndim == 2 and vals.shape[0] == n_feat and vals.shape[1] != n_feat:\n",
        "                vals = vals.T\n",
        "            # Ensure 2D (n_samples, n_features)\n",
        "            if vals.ndim != 2 or vals.shape[1] != n_feat:\n",
        "                vals = np.atleast_2d(vals)\n",
        "                if vals.shape[1] == n_feat:\n",
        "                    pass\n",
        "                elif vals.shape[0] == n_feat:\n",
        "                    vals = vals.T\n",
        "                else:\n",
        "                    vals = np.atleast_2d(vals.mean(axis=tuple(range(vals.ndim - 1))))\n",
        "                    if vals.shape[1] != n_feat:\n",
        "                        m = min(vals.shape[1], n_feat)\n",
        "                        tmp = np.zeros((1, n_feat), dtype=vals.dtype); tmp[0, :m] = vals[0, :m]\n",
        "                        vals = tmp\n",
        "        # ===========================================================\n",
        "\n",
        "        imp = pd.Series(vals.mean(axis=0), index=fn).sort_values(ascending=False)\n",
        "        per_model_top10[name] = imp.head(10)\n",
        "\n",
        "        (imp.head(EXPLAIN_TOP_BAR).sort_values(ascending=True)\n",
        "            .plot(kind=\"barh\", figsize=(6,8)))\n",
        "        plt.tight_layout(); plt.savefig(EXPLAIN_DIR/f\"base_{name}_bar.png\", dpi=200); plt.close()\n",
        "\n",
        "        for f, v in imp.items(): global_feat_importance[f].append(v)\n",
        "        print(f\"✓ SHAP computed for base model: {name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! SHAP failed for base model {name}: {e}\")\n",
        "\n",
        "# Aggregate across models\n",
        "if global_feat_importance:\n",
        "    agg = {f: np.mean(vs) for f, vs in global_feat_importance.items()}\n",
        "    agg = pd.Series(agg).sort_values(ascending=False)\n",
        "    top10 = agg.head(10)\n",
        "    pd.DataFrame(per_model_top10).fillna(0.0).to_csv(EXPLAIN_DIR/\"per_model_top10.csv\")\n",
        "    agg.to_csv(EXPLAIN_DIR/\"global_feature_importance.csv\", header=[\"mean_abs_shap\"])\n",
        "    (top10.sort_values(ascending=True).plot(kind=\"barh\", figsize=(5,4)))\n",
        "    plt.tight_layout(); plt.savefig(EXPLAIN_DIR/\"global_top10_bar.png\", dpi=200); plt.close()\n",
        "    print(\"✓ Saved per-model & aggregated feature importances.\")\n",
        "\n",
        "print(\"\\n=== OUTPUTS ===\")\n",
        "print(f\"- Bundle: {OUT_DIR/'stacking_bundle.pkl'}\")\n",
        "print(f\"- Metrics: {OUT_DIR/'metrics.json'}\")\n",
        "print(f\"- Explain dir: {EXPLAIN_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "147a4922a0914933a38396f809a3de46",
            "04754e03613f4392945ef63acd11d1fd",
            "0efe3cbe56244280a162414a4fd4dbf0",
            "ec3891872b2b45afa518c22c8488f321",
            "b9f96078a7bf48b8b1e49546d254f222",
            "941aa790aff04172a72bb4184433553a",
            "734de97d46cf46828febccd640662d51",
            "e86340a6dbc849afa2bc8369bddcde5c",
            "1f0a79cd3f904048b760936ea646c601",
            "fd72847af3ee450691d0a14b856725ca",
            "9bf3e14bd0354287ba28bb785d756673"
          ]
        },
        "id": "iV6--nIo8NXN",
        "outputId": "594d82b2-868a-4cc3-a967-780154ab3265"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found dataset at: /content/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Base learners: ['xgb', 'lgb', 'cat', 'rf', 'hgb']\n",
            "→ Rebuilding OOF & test matrices ...\n",
            "[xgb] OOF ROC-AUC=0.8304 | PR-AUC=0.6420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lgb] OOF ROC-AUC=0.8230 | PR-AUC=0.6232\n",
            "[cat] OOF ROC-AUC=0.8365 | PR-AUC=0.6508\n",
            "[rf] OOF ROC-AUC=0.8214 | PR-AUC=0.6148\n",
            "[hgb] OOF ROC-AUC=0.8255 | PR-AUC=0.6276\n",
            "Meta holdout ROC-AUC=0.8384 | PR-AUC=0.6434\n",
            "✓ Saved: outputs/stacking_bundle.pkl\n",
            "✓ Saved meta SHAP summaries & importances.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "147a4922a0914933a38396f809a3de46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: xgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|=================== | 771/800 [00:18<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: lgb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|===================| 790/800 [00:18<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1596/1600 [04:19<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP computed for base model: rf\n",
            "✓ SHAP computed for base model: hgb\n",
            "✓ Saved per-model & aggregated feature importances.\n",
            "\n",
            "=== OUTPUTS ===\n",
            "- Bundle: outputs/stacking_bundle.pkl\n",
            "- Metrics: outputs/metrics.json\n",
            "- Explain dir: outputs/explain\n"
          ]
        }
      ]
    }
  ]
}